[
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#objective",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#objective",
    "title": "Matchup satellite data to track locations",
    "section": "Objective",
    "text": "Objective\nThis tutorial will demonstrate how to extract satellite data around a set of points defined by longitude, latitude, and time coordinates, like those produced by an animal telemetry tag, and ship track, or a glider track."
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#the-tutorial-demonstrates-the-following-techniques",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#the-tutorial-demonstrates-the-following-techniques",
    "title": "Matchup satellite data to track locations",
    "section": "The tutorial demonstrates the following techniques",
    "text": "The tutorial demonstrates the following techniques\n\nImporting track data in csv file to data frame\nPlotting the latitude/longitude points onto a map\nUsing the rerddapXtraco function to extract satellite data from an ERDDAP data server along a track\nPlotting the satellite data onto a map"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#datasets-used",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#datasets-used",
    "title": "Matchup satellite data to track locations",
    "section": "Datasets used",
    "text": "Datasets used\nChlorophyll a concentration, the European Space Agency’s Ocean Colour Climate Change Initiative (OC-CCI) Monthly dataset v6.0\nWe’ll use the European Space Agency’s OC-CCI product (https://climate.esa.int/en/projects/ocean-colour/) to obtain chlorophyll data. This is a merged product combining data from many ocean color sensors to create a long time series (1997-present).\nLoggerhead turtle telemetry track data\nThe turtle was raised in captivity in Japan, then tagged and released on 05/04/2005 in the Central Pacific. Its tag transmitted for over 3 years and went all the way to the Southern tip of Baja California. This data set has been subsampled to reduce the data requests needed for this tutorial from over 1200 to 25. The track data are stored in the data folder in this project folder."
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#install-and-load-packages",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#install-and-load-packages",
    "title": "Matchup satellite data to track locations",
    "section": "Install and load packages",
    "text": "Install and load packages\n\n# Function to check if pkgs are installed, and install any missing pkgs\npkgTest &lt;- function(x)\n{\n  if (!require(x,character.only = TRUE))\n  {\n    install.packages(x,dep=TRUE,repos='http://cran.us.r-project.org')\n    if(!require(x,character.only = TRUE)) stop(x, \" :Package not found\")\n  }\n}\n\n# Create list of required packages\nlist.of.packages &lt;- c(\"rerddap\", \"plotdap\", \"parsedate\", \"ggplot2\", \"rerddapXtracto\",\n                      \"date\", \"maps\", \"mapdata\", \"RColorBrewer\",\"viridis\")\n\n# Create list of installed packages\npkges = installed.packages()[,\"Package\"]\n\n# Install and load all required pkgs\nfor (pk in list.of.packages) {\n  pkgTest(pk)\n}"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#import-the-track-data-into-a-data-frame",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#import-the-track-data-into-a-data-frame",
    "title": "Matchup satellite data to track locations",
    "section": "Import the track data into a data frame",
    "text": "Import the track data into a data frame\n\n# Import csv file into a data frame\nturtle_df &lt;- read.csv(\"../data/25317_05_subsampled.dat\")\n# Show 3 rows from the data frame\nhead(turtle_df,3)\n\n  mean_lon mean_lat year month day\n1 176.6194 32.67873 2005     5   4\n2 175.8609 35.05773 2005     6  23\n3 180.5926 40.40576 2005     8  12"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#plot-the-track-on-a-map",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#plot-the-track-on-a-map",
    "title": "Matchup satellite data to track locations",
    "section": "Plot the track on a map",
    "text": "Plot the track on a map\n\n# Download world map\nmapWorld &lt;- map_data(\"world\", wrap=c(0,360))\n\n# Map turtle tracks\nggplot(turtle_df, aes(mean_lon,mean_lat)) +\n  geom_path(group=1)+\n  geom_point(aes(x=mean_lon,y=mean_lat), pch=1, size=2 )+\n  geom_point(aes(x=mean_lon[1],y=mean_lat[1]),fill=\"green\", shape=24, size=3)+\n  geom_point(aes(x=mean_lon[length(mean_lon)],y=mean_lat[length(mean_lat)]), shape=22, size=3, fill=\"red\")+\n  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group)) + \n  coord_fixed(xlim = c(120,260),ylim = c(15,60))+\n  labs(x=\"Longitude (deg)\", y=\"Latitude (deg)\", title=\"Turtle Track with start (green) and end location (red)\")+\n  theme(plot.title=element_text(hjust=0.5), aspect.ratio=1/2)\n\n\n\n\n\n\n\n\nIn this exercise, two different ways of extracting data from ERDDAP data server along a track of xyt points are demonstrated:\n\nUsing the rerddapXtracto package which was written specifically for this task\nBy manually constructing a URL with the data data request\n\n\nExtracting XYT data using the rerddapXtracto package\nWe will use the `rxtracto function of the rerddapXtracto package, which was written to simplify data extraction from ERDDAP servers.\nLet’s use data from the monthly product of the OC-CCI datasets.\nThe ERDDAP URL to the monthly product is below:\nhttps://oceanwatch.pifsc.noaa.gov/erddap/griddap/esa-cci-chla-monthly-v6-0\nA note on dataset selection\n\nWe have preselected the dataset because we know it will work with this exercise. If you were selecting datasets on your own, you would want to check out the dataset to determine if its spatial and temporal coveragea are suitable for your application. Following the link above you will find:\nThe latitude range is -89.97916 to 89.97916 and the longitude range is 0.020833 to 359.97916, which covers the track latitude range of 23.72 to 41.77 and longitude range of 175.86 to 248.57.\nThe time range is 1997-09-04 to 2023-12-01 (at the day of this writing), which covers the track time range of 2005-05-04 to 2008-08-16.\nYou should also note the name of the variable you will be downloading. For this dataset it is “chlor_a”\n\n# Set dataset ID\ndataset &lt;- 'esa-cci-chla-monthly-v6-0'\n\n# Get data information from ERDDAP server\ndataInfo &lt;- rerddap::info(dataset, url= \"https://oceanwatch.pifsc.noaa.gov/erddap\")"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#examine-metadata",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#examine-metadata",
    "title": "Matchup satellite data to track locations",
    "section": "Examine metadata",
    "text": "Examine metadata\nrerddap::info returns the metadata of the requested dataset. We can first understand the attributes dataInfo includes then examine each attribute.\n\n# Display the metadata\ndataInfo\n\n&lt;ERDDAP info&gt; esa-cci-chla-monthly-v6-0 \n Base URL: https://oceanwatch.pifsc.noaa.gov/erddap \n Dataset Type: griddap \n Dimensions (range):  \n     time: (1997-09-04T00:00:00Z, 2023-12-01T00:00:00Z) \n     latitude: (-89.97916666666666, 89.97916666666667) \n     longitude: (0.020833333333314386, 359.97916666666663) \n Variables:  \n     chlor_a: \n         Units: mg m-3 \n     chlor_a_log10_bias: \n     chlor_a_log10_rmsd: \n     MERIS_nobs_sum: \n     MODISA_nobs_sum: \n     OLCI_A_nobs_sum: \n     OLCI_B_nobs_sum: \n     SeaWiFS_nobs_sum: \n     total_nobs_sum: \n     VIIRS_nobs_sum: \n\n# Display data attributes\nnames(dataInfo)\n\n[1] \"variables\" \"alldata\"   \"base_url\" \n\n# Examine attribute: variables\ndataInfo$variables\n\n        variable_name data_type actual_range\n1             chlor_a     float             \n2  chlor_a_log10_bias     float             \n3  chlor_a_log10_rmsd     float             \n4      MERIS_nobs_sum     float             \n5     MODISA_nobs_sum     float             \n6     OLCI_A_nobs_sum     float             \n7     OLCI_B_nobs_sum     float             \n8    SeaWiFS_nobs_sum     float             \n9      total_nobs_sum     float             \n10     VIIRS_nobs_sum     float             \n\n# Distribute attributes of dataInfo$alldata\nnames(dataInfo$alldata)\n\n [1] \"NC_GLOBAL\"          \"time\"               \"latitude\"          \n [4] \"longitude\"          \"chlor_a\"            \"MERIS_nobs_sum\"    \n [7] \"MODISA_nobs_sum\"    \"OLCI_A_nobs_sum\"    \"OLCI_B_nobs_sum\"   \n[10] \"SeaWiFS_nobs_sum\"   \"VIIRS_nobs_sum\"     \"chlor_a_log10_bias\"\n[13] \"chlor_a_log10_rmsd\" \"total_nobs_sum\"    \n\n\n\nExtract data using the rxtracto function\nFirst we need to define the bounding box within which to search for coordinates. The rxtracto function allows you to set the size of the box used to collect data around the track points using the xlen and ylen arguments. The values for xlen and ylen are in degrees. For our example, we can use 0.2 degrees for both arguments. Note: You can also submit vectors for xlen and ylen, as long as they are the same length as xcoord, ycoord, and tcoord if you want to set a different search radius around each track point.\n\n# Set the variable we want to extract data from:\nparameter &lt;- 'chlor_a'\n\n# Set xlen, ylen to 0.2 degree\nxlen &lt;- 0.2 \nylen &lt;- 0.2\n\n# Create date column using year, month and day in a format ERDDAP will understand (eg. 2008-12-15)\nturtle_df$date &lt;- as.Date(paste(turtle_df$year, turtle_df$month, turtle_df$day, sep=\"-\"))\n\n# Get variables x, y, t coordinates from turtle track data\nxcoords &lt;- turtle_df$mean_lon\nycoords &lt;- turtle_df$mean_lat\ntcoords &lt;- turtle_df$date\n\n# Extract satellite data using x, y, t coordinates from turtle track data\nchl_track &lt;- rxtracto(dataInfo, \n                      parameter=parameter, \n                      xcoord=xcoords, ycoord=ycoords, \n                      tcoord=tcoords, xlen=xlen, ylen=ylen)"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#check-the-output-of-the-rxtracto-function",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#check-the-output-of-the-rxtracto-function",
    "title": "Matchup satellite data to track locations",
    "section": "Check the output of the rxtracto function",
    "text": "Check the output of the rxtracto function\n\n# Check all variables extracted using rxtracto\nchl_track\n\n$`mean chlor_a`\n [1] 0.26779765 0.12073122 0.30777924 0.31780368 0.28884829 0.36146353\n [7] 0.22923882 0.11644071 0.09268904 0.05536651 0.18447913 0.22765385\n[13] 0.23869553 0.24669819 0.35838123 0.09624625 0.12024124 0.11400095\n[19] 0.10017463 0.09397742 0.08515869 0.06913812 0.14883095 0.51560753\n[25] 0.65806269\n\n$`stdev chlor_a`\n [1] 0.032191816 0.007231171 0.036716832 0.041052758 0.027952051 0.053364804\n [7] 0.024394244 0.007282479 0.003880810 0.001490017 0.040774493 0.025363286\n[13] 0.014782180 0.013523678 0.036640145 0.004842596 0.004098601 0.003910613\n[19] 0.003537558 0.005928580 0.007001476 0.004942355 0.011980482 0.099209610\n[25] 0.149563991\n\n$n\n [1] 36 36 36 27 36 30 36 36 30 30 30 30 36 30 36 30 36 36 36 30 36 30 36 36 36\n\n$`satellite date`\n [1] \"2005-05-01T00:00:00Z\" \"2005-07-01T00:00:00Z\" \"2005-08-01T00:00:00Z\"\n [4] \"2005-10-01T00:00:00Z\" \"2005-12-01T00:00:00Z\" \"2006-01-01T00:00:00Z\"\n [7] \"2006-03-01T00:00:00Z\" \"2006-05-01T00:00:00Z\" \"2006-06-01T00:00:00Z\"\n[10] \"2006-08-01T00:00:00Z\" \"2006-09-01T00:00:00Z\" \"2006-11-01T00:00:00Z\"\n[13] \"2007-01-01T00:00:00Z\" \"2007-02-01T00:00:00Z\" \"2007-04-01T00:00:00Z\"\n[16] \"2007-06-01T00:00:00Z\" \"2007-07-01T00:00:00Z\" \"2007-09-01T00:00:00Z\"\n[19] \"2007-11-01T00:00:00Z\" \"2007-12-01T00:00:00Z\" \"2008-02-01T00:00:00Z\"\n[22] \"2008-04-01T00:00:00Z\" \"2008-05-01T00:00:00Z\" \"2008-07-01T00:00:00Z\"\n[25] \"2008-08-01T00:00:00Z\"\n\n$`requested lon min`\n [1] 176.5194 175.7609 180.4926 183.4102 186.8997 193.2152 198.9158 196.2679\n [9] 194.2116 192.7545 193.9788 191.9444 191.6600 194.9631 199.2066 205.5050\n[17] 210.1805 215.6225 222.9073 225.5386 232.3064 239.4530 245.7716 248.4710\n[25] 245.6579\n\n$`requested lon max`\n [1] 176.7194 175.9609 180.6926 183.6102 187.0997 193.4152 199.1158 196.4679\n [9] 194.4116 192.9545 194.1788 192.1444 191.8600 195.1631 199.4066 205.7050\n[17] 210.3805 215.8225 223.1073 225.7386 232.5064 239.6530 245.9716 248.6710\n[25] 245.8579\n\n$`requested lat min`\n [1] 32.57873 34.95773 40.30576 41.58480 37.26623 32.03793 32.01126 34.81224\n [9] 34.59661 36.99175 41.66933 38.12796 34.63858 31.63964 34.24324 35.02771\n[17] 38.36083 39.23749 35.76793 30.08540 28.22859 25.88108 24.73662 23.62417\n[25] 26.68177\n\n$`requested lat max`\n [1] 32.77873 35.15773 40.50576 41.78480 37.46623 32.23793 32.21126 35.01224\n [9] 34.79661 37.19175 41.86933 38.32796 34.83858 31.83964 34.44324 35.22771\n[17] 38.56083 39.43749 35.96793 30.28540 28.42859 26.08108 24.93662 23.82417\n[25] 26.88177\n\n$`requested z min`\n [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n\n$`requested z max`\n [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n\n$`requested date`\n [1] \"2005-05-04\" \"2005-06-23\" \"2005-08-12\" \"2005-10-01\" \"2005-11-20\"\n [6] \"2006-01-09\" \"2006-02-28\" \"2006-04-19\" \"2006-06-08\" \"2006-07-28\"\n[11] \"2006-09-16\" \"2006-11-05\" \"2006-12-25\" \"2007-02-13\" \"2007-04-04\"\n[16] \"2007-05-24\" \"2007-07-13\" \"2007-09-01\" \"2007-10-21\" \"2007-12-10\"\n[21] \"2008-01-29\" \"2008-03-19\" \"2008-05-08\" \"2008-06-27\" \"2008-08-16\"\n\n$`median chlor_a`\n [1] 0.26985641 0.11935977 0.31413330 0.31312889 0.28226255 0.35456662\n [7] 0.22596541 0.11711950 0.09256660 0.05540323 0.18355133 0.22467585\n[13] 0.24202415 0.24848759 0.34953472 0.09518800 0.11983451 0.11379882\n[19] 0.10012896 0.09296544 0.08735247 0.06766215 0.14883141 0.49834299\n[25] 0.67895702\n\n$`mad chlor_a`\n [1] 0.030100095 0.008344179 0.039171724 0.027213317 0.023952735 0.049566912\n [7] 0.022928175 0.009027836 0.003463391 0.001267676 0.035848973 0.026539111\n[13] 0.016244819 0.011793729 0.035606685 0.003954114 0.002984454 0.002953619\n[19] 0.002999692 0.003767908 0.003242842 0.003565519 0.015092995 0.109981245\n[25] 0.133971250\n\nattr(,\"row.names\")\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\"\nattr(,\"class\")\n[1] \"list\"          \"rxtractoTrack\"\nattr(,\"base_url\")\n[1] \"https://oceanwatch.pifsc.noaa.gov/erddap/\"\nattr(,\"datasetid\")\n[1] \"esa-cci-chla-monthly-v6-0\"\n\n\nrxtracto computes statistics using all the pixels found in the search radius around each track point."
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#plotting-the-results-using-plottrack",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#plotting-the-results-using-plottrack",
    "title": "Matchup satellite data to track locations",
    "section": "Plotting the results using plotTrack",
    "text": "Plotting the results using plotTrack\nWe will use the “plotTrack” function to plot the results. “plotTrack” is a function of the “rerddapXtracto” package designed specifically to plot the results of the “rxtracto” function. It provides an easy way to make a quick plot, however it’s not very customizable.\n\n# Plot tracks with color: algae specifically designed for chlorophyll\nplotTrack(chl_track, xcoords, ycoords, tcoords, size=3, plotColor = 'viridis')"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#animating-the-track",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#animating-the-track",
    "title": "Matchup satellite data to track locations",
    "section": "Animating the track",
    "text": "Animating the track\nOne of the nice features of the “plotTrack” function is that it is very easy to make an animation of the track data. This will take a minute to run. It creates an animated gif that will display in the Rstudio viewer window once the encoding to gif is done.\n\n# Animate tracks\n\nmake180 &lt;- function(lon) {\n  ind &lt;- which(lon &gt; 180)\n  lon[ind] &lt;- lon[ind] - 360\n  return(lon)\n}\n\nplotTrack(chl_track, make180(xcoords), ycoords, tcoords, plotColor = 'viridis',\n          animate = TRUE, cumulative = TRUE)\n\nNULL"
  },
  {
    "objectID": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#plotting-the-results-using-ggplot",
    "href": "tutorials/r/4-matchup-satellite-data-to-track-locations.html#plotting-the-results-using-ggplot",
    "title": "Matchup satellite data to track locations",
    "section": "Plotting the results using ggplot",
    "text": "Plotting the results using ggplot\n\nCreate a data frame with the turtle track and the output of rxtracto\nIf we to do an customization of the plot, its better to plot the dat ausing ggplot. We will first create a data frame that contains longitudes and latitudes from the turtle and associated satellite chlor-a values.\n\n# Create a data frame of coords from turtle and chlor_a values \nnew_df &lt;- as.data.frame(cbind(xcoords, ycoords,  \n                              chl_track$`requested lon min`, \n                              chl_track$`requested lon max`, \n                              chl_track$`requested lat min`, \n                              chl_track$`requested lon max`,  \n                              chl_track$`mean chlor_a`))\n\n# Set variable names\nnames(new_df) &lt;- c(\"Lon\", \"Lat\", \"Matchup_Lon_Lower\", \"Matchup_Lon_Upper\", \"Matchup_Lat_Lower\", \"Matchup_Lat_Upper\",  \"Chlor_a\")\nwrite.csv(new_df, \"matchup_df.csv\")\n\n\n\nPlot using ggplot\n\n# Import world map\nmapWorld &lt;- map_data(\"world\", wrap=c(0,360))\n\n# Draw the track positions with associated chlora values\nggplot(new_df) +\n  geom_point(aes(Lon,Lat,color=log(Chlor_a))) +\n  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group)) + \n  coord_fixed(xlim = c(120,260),ylim = c(15,60)) +\n  scale_color_viridis(discrete = FALSE) +\n  labs(x=\"Longitude (deg)\", y=\"Latitude (deg)\", title=\"Turtle Track with chlor-a values\")+\n  theme(plot.title=element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\n\nExtracting XYT data by constructing the URL data requests manually\nFirst we need to set up the ERDDAP URL using the datasets ID and the name of the variable we are interested in. Note that we are requesting the data as .csv\ndata_url = \"https://oceanwatch.pifsc.noaa.gov/erddap/griddap/aqua_chla_1d_2018_0.csv?chlor_a\"\nIdeally, we would work with daily data since we have one location per day. But chlorophyll data is severely affected by clouds (i.e. lots of missing data), so you might need to use weekly or even monthly data to get sufficient non-missing data. We will start with the monthly chl-a data since it contains fewer gaps.\n\n# Set erddap address\nerddap &lt;- \"https://oceanwatch.pifsc.noaa.gov/erddap/griddap/aqua_chla_monthly_2018_0.csv?chlor_a\"\n\n# Get longitude and latitude from turtle track data\nlon &lt;- turtle_df$mean_lon\nlat &lt;- turtle_df$mean_lat\n\n# Get time from turtle track data and convert into ERDDAP date format\ndates &lt;- mdy.date(turtle_df$month,turtle_df$day,turtle_df$year)\ndates2 &lt;- format(as.Date(dates), \"%Y-%m-%d\")\n\n# Initatilize tot variable where data will be downloaded to\ntot &lt;- rep(NA, 4)\n\n# Loop through each turtle track data\nfor (i in 1:dim(turtle_df)[1]) {\n  \n  # Create erddap URL by adding lat, lon, dates of each track point \n  url &lt;-  paste(erddap, \"[(\", dates2[i], \"):1:(\", dates2[i], \")][(\", lat[i], \"):1:(\", lat[i], \")][(\", lon[i], \"):1:(\", lon[i], \")]\", sep = \"\")  \n  \n  # Request and load satelite data from ERDDAP\n  new &lt;- read.csv(url, skip=2, header = FALSE) \n  \n  # Append the data\n  tot &lt;- rbind(tot, new)   \n}\n\n# Delete the first row (default column names)\ntot &lt;- tot[-1, ]\n\n# Rename columns\nnames(tot) &lt;- c(\"chlo_date\", \"matched_lat\", \"matched_lon\", \"matched_chl.m\")\n\n# Create data frame combining turtle track data and the chlo-a data\nchl_track2 &lt;- data.frame(turtle_df, tot)\n\n# Write the data frame to csv file\nwrite.csv(chl_track2, 'turtle-track-chl.m.csv', row.names = FALSE)\n\n\n\nMake a map of the data extracted using the second method\n\n# Draw the track positions with associated chlora values\nggplot(chl_track2) +\n  geom_point(aes(mean_lon,mean_lat,color=log(matched_chl.m))) +\n  geom_polygon(data = mapWorld, aes(x=long, y = lat, group = group)) + \n  coord_fixed(xlim = c(120,260),ylim = c(15,60)) +\n  scale_color_viridis(discrete = FALSE) +\n  labs(x=\"Longitude (deg)\", y=\"Latitude (deg)\", title=\"Turtle Track with chlor-a values\")+\n  theme(plot.title=element_text(hjust=0.5))\n\n\n\n\n\n\n\n\n\n\nPlot histogram of chlorophyll\nHow do the chlorophyll values of the turtle track compare to values in the surrounding environment? Meaning does the turtle seem to have a preference for certain chlorophyll values? To look at this we will plot a histograms of the track chl valuesand those of the surrounding area.\nFirst we will get a 3D block of chl data from the region and of the turtle track over the span of time the turtle was in that area. We will use the ‘xtracto_3d’ function of rerddapXtracto to get the data. This data call will take a few minutes.\n\nchl_grid &lt;- rxtracto_3D(dataInfo, \n                        parameter=parameter, \n                        xcoord=c(min(xcoords),max(xcoords)), \n                        ycoord=c(min(ycoords),max(ycoords)), \n                        tcoord=c(min(tcoords),max(tcoords)))\n\nchl_area &lt;- as.vector(chl_grid$chlor_a) \n\n# remove NA values \nchl_area &lt;- chl_area[!is.na(chl_area)]\n\n# vector or turtle chlorophyll \n\nchl_turtle &lt;- chl_track$`mean chlor_a`\n\nNow we we plot histograms of all the chlorphyll values in the area, and those of the turtle track. Since we subset the turtletrack, and only have 25 points for this subsampopled dataset the turtle histogram isn’t as useful as it would be with a larger dataset.\n\nggplot(as.data.frame(chl_area)) + \n  geom_histogram(aes(x=chl_area,y=after_stat(density),color = \"darkgray\",fill='Area'),color='black', bins=50) + \n  geom_histogram(data=as.data.frame(chl_turtle), aes(x=chl_turtle,y=after_stat(density),color='green', fill='Turtle'),color='black',bins=50, alpha=.4) + \n  scale_x_continuous(limits = c(0,.9), expand = c(0, 0)) + \n  scale_y_continuous(limits = c(0,15), expand = c(0, 0)) +\n  labs(x='Chlorophyll values',y='Density') + \n  theme_bw() + \n  scale_fill_manual(values=c(\"darkgray\",\"green\"),'')\n\n\n\n\n\n\n\n\n\nExercise 1:\nRepeat the steps above with a different dataset. For example, extract sea surface temperature data using the following dataset: https://coastwatch.pfeg.noaa.gov/erddap/griddap/nesdisGeoPolarSSTN5NRT_Lon0360.html \\* This dataset is a different ERDDAP, so remember to change the base URL. \\* Set the new dataset ID and variable name.\n\n\nExercise 2:\nGo to an ERDDAP of your choice, find a dataset of interest, generate the URL, copy it and edit the script above to run a match up on that dataset. To find other ERDDAP servers, you can use this search engine: http://erddap.com/ \\* This dataset will likely be on a different ERDDAP, so remember to change the base URL. \\* Set the new dataset ID and variable name. \\* Check the metadata to make sure the dataset covers the spatial and temporal range of the track dataset.\n\n\nOptional\nRepeat the steps above with a daily version of the OC-CCI dataset to see how cloud cover can reduce the data you retrieve. https://coastwatch.pfeg.noaa.gov/erddap/griddap/pmlEsaCCI60OceanColorDaily_Lon0360.html"
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#summary",
    "href": "tutorials/r/2-subset-and-plot.html#summary",
    "title": "Subset and Plot",
    "section": "Summary",
    "text": "Summary\nIn this example, we will utilize the earthdatalogin R package to retrieve, subset, and crop sea surface temperature data as a file and as a datacube from NASA Earthdata search. The earthdatalogin R package simplifies the process of discovering and accessing NASA Earth science data.\nFor more on earthdatalogin visit the earthdatalogin GitHub page and/or the earthdatalogin documentation site. Be aware that earthdatalogin is under active development and that we are using the development version on GitHub.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#terminology",
    "href": "tutorials/r/2-subset-and-plot.html#terminology",
    "title": "Subset and Plot",
    "section": "Terminology",
    "text": "Terminology\n\nZarr files: is a community project to develop specifications and software for storage of large N-dimensional typed arrays, also commonly known as tensors. A particular focus of Zarr is to provide support for storage using distributed systems like cloud object stores, and to enable efficient I/O for parallel computing applications. Learn more here.\nOpen Data Cube (ODC): is an Open Source Geospatial Data Management and Analysis Software project that helps you harness the power of Satellite data. At its core, the ODC is a set of Python libraries and PostgreSQL database that helps you work with geospatial raster data. The ODC seeks to increase the value and impact of global Earth observation satellite data by providing an open and freely accessible exploitation architecture. Learn more here.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#prerequisites",
    "href": "tutorials/r/2-subset-and-plot.html#prerequisites",
    "title": "Subset and Plot",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe tutorials today can be run with the guest Earthdata Login that is in earthdatalogin. However, if you will be using the NASA Earthdata portal more regularly, please register for an Earthdata Login account. Please https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#load-required-packages",
    "href": "tutorials/r/2-subset-and-plot.html#load-required-packages",
    "title": "Subset and Plot",
    "section": "Load Required Packages",
    "text": "Load Required Packages\nWe are using the JupyterHub and all necessary packages are already installed for you.\nNote: See the set-up tab (in left nav bar) for instructions on getting set up on your own computer, but be aware that it is common to run into trouble getting GDAL set up properly to handle netCDF files. Using a Docker image (and Python) is often less aggravating.\n\nlibrary(earthdatalogin)\nlibrary(lubridate)\nlibrary(terra)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#get-a-vector-of-urls-to-our-nc-files",
    "href": "tutorials/r/2-subset-and-plot.html#get-a-vector-of-urls-to-our-nc-files",
    "title": "Subset and Plot",
    "section": "Get a vector of urls to our nc files",
    "text": "Get a vector of urls to our nc files\nAuthenticate.\n\nearthdatalogin::edl_netrc() \n\nGet the urls. The results object is a vector of urls pointing to our netCDF files in the cloud. Each netCDF file is circa 670Mb.\n\nshort_name &lt;- 'MUR-JPL-L4-GLOB-v4.1'\nbbox &lt;- c(xmin=-75.5, ymin=33.5, xmax=-73.5, ymax=35.5) \ntbox &lt;- c(\"2020-01-16\", \"2020-12-16\")\n\nresults &lt;- earthdatalogin::edl_search(\n  short_name = short_name,\n  version = \"4.1\",\n  temporal = tbox, \n  bounding_box = paste(bbox,collapse=\",\")\n)\nlength(results)\n\n[1] 336\n\nresults[1:3]\n\n[1] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200116090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n[2] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200117090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n[3] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200118090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#crop-and-plot-one-netcdf-file",
    "href": "tutorials/r/2-subset-and-plot.html#crop-and-plot-one-netcdf-file",
    "title": "Subset and Plot",
    "section": "Crop and plot one netCDF file",
    "text": "Crop and plot one netCDF file\nEach MUR SST netCDF file is large so I do not want to download. Instead I will use terra::rast() to do subset the data on the server side. vsi = TRUE is letting function know that these are files in the cloud and to use GDAL functionality for that type of resource.\n\nras &lt;- terra::rast(results[1], vsi=TRUE)\n\nGetting errors? Scroll below to the troubleshooting section.\nCrop to a very small region.\n\n# note order of terms is different than in bbox!!\ne &lt;- terra::ext(c(xmin=-75.5, xmax=-73.5,  ymin=33.5, ymax=35.5 ))\nrc &lt;- terra::crop(ras, e)\nrc\n\nclass       : SpatRaster \ndimensions  : 200, 200, 6  (nrow, ncol, nlyr)\nresolution  : 0.01, 0.01  (x, y)\nextent      : -75.495, -73.495, 33.505, 35.505  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nnames       : analysed_sst, analy~error, mask, sea_i~ction, dt_1km_data, sst_anomaly \nmin values  :      290.656,        0.37,    1,        1.28,         128,       0.410 \nmax values  :      298.954,        0.40,    2,        1.28,         128,       3.585 \ntime        : 2020-01-16 09:00:00 UTC \n\n\nPlot:\n\nplot(rc[[c(1, 2)]])",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#crop-and-plot-multiple-netcdf-files",
    "href": "tutorials/r/2-subset-and-plot.html#crop-and-plot-multiple-netcdf-files",
    "title": "Subset and Plot",
    "section": "Crop and plot multiple netCDF files",
    "text": "Crop and plot multiple netCDF files\nWe can send multiple urls to terra.\n\nras_all &lt;- terra::rast(results[c(1:4)], vsi = TRUE)\nras_all\n\nclass       : SpatRaster \ndimensions  : 17999, 36000, 24  (nrow, ncol, nlyr)\nresolution  : 0.01, 0.01  (x, y)\nextent      : -179.995, 180.005, -89.995, 89.995  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsources     : 20200116090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc:analysed_sst  \n              20200116090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc:analysis_error  \n              20200116090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc:mask  \n              ... and 21 more source(s)\nvarnames    : analysed_sst (analysed sea surface temperature) \n              analysis_error (estimated error standard deviation of analysed_sst) \n              mask (sea/land field composite mask) \n              ...\nnames       : analysed_sst, analy~error, mask, sea_i~ction, dt_1km_data, sst_anomaly, ... \nunit        :       kelvin,      kelvin,     ,            ,       hours,      kelvin, ... \ntime        : 2020-01-16 09:00:00 to 2020-01-19 09:00:00 UTC \n\n\nCrop to a small extent. Note order of terms is different than in bbox! Since we will only plot sst for this example, it is faster to first select our variable of interest.\n\ne &lt;- terra::ext(c(xmin=-75.5, xmax=-73.5,  ymin=33.5, ymax=35.5 ))\nras_sst &lt;- ras_all[\"analysed_sst\",]\nrc_sst &lt;- terra::crop(ras_sst, e)\nrc_sst\n\nclass       : SpatRaster \ndimensions  : 200, 200, 4  (nrow, ncol, nlyr)\nresolution  : 0.01, 0.01  (x, y)\nextent      : -75.495, -73.495, 33.505, 35.505  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nvarname     : analysed_sst (analysed sea surface temperature) \nnames       : analysed_sst, analysed_sst, analysed_sst, analysed_sst \nmin values  :      290.656,      290.316,      289.456,      289.216 \nmax values  :      298.954,      298.869,      298.662,      298.119 \nunit        :       kelvin,       kelvin,       kelvin,       kelvin \ntime        : 2020-01-16 09:00:00 to 2020-01-19 09:00:00 UTC \n\n\nConvert Kelvin to Celsius.\n\nrc_sst &lt;- rc_sst - 273.15\n\nNow plot. We will set the range so it is the same across plots and clean up the titles to be just day without time.\n\ntitles &lt;- terra::time(x = rc_sst) |&gt; lubridate::date() |&gt; as.character()\nplot(rc_sst, \n     range = c(16, 26),\n     main = titles)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#reading-in-a-zarr-file",
    "href": "tutorials/r/2-subset-and-plot.html#reading-in-a-zarr-file",
    "title": "Subset and Plot",
    "section": "Reading in a Zarr file",
    "text": "Reading in a Zarr file\nReading in Zarr files is easy in Python with xarray but currently this is difficult in R. See the gdalcubes.qmd file in the tutorials/r directory of this GitHub repository. However we can open individual files from a Zarr file.\nRead one file.\n\nurl &lt;- \"https://mur-sst.s3.us-west-2.amazonaws.com/zarr-v1\"\nprefixes &lt;- 'ZARR:\\\"/vsicurl/'\nslice &lt;- '\\\":/analysed_sst:0\"'\naddr &lt;- paste0(prefixes, url, slice)\ny = terra::rast(addr)\n\nPlot.\n\ne &lt;- terra::ext(c(xmin=-75.5, xmax=-73.5,  ymin=33.5, ymax=35.5 ))\ny |&gt; terra::crop(e) |&gt; terra::plot()\n\n\n\n\n\n\n\n\nRead multiple files.\n\nvrt &lt;- function(i) {\n  prefix &lt;-  'ZARR:\\\"/vsicurl/'\n  url &lt;- \"https://mur-sst.s3.us-west-2.amazonaws.com/zarr-v1\"\n  slice &lt;- paste0('\\\":/analysed_sst:',i,'\"')\n  paste0(prefix, url, slice)\n}\n\n\ny &lt;- terra::rast(vrt(0:3))\ne &lt;- terra::ext(c(xmin=-75.5, xmax=-73.5,  ymin=33.5, ymax=35.5 ))\ny |&gt; terra::crop(e) |&gt; terra::plot()",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/r/2-subset-and-plot.html#conclusions",
    "href": "tutorials/r/2-subset-and-plot.html#conclusions",
    "title": "Subset and Plot",
    "section": "Conclusions",
    "text": "Conclusions\nSome really cool things just happened here! You connected to multiple remote-sensing files (netCDF) in the cloud and worked with them without directly downloading them.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/python/4-data-cubes.html#summary",
    "href": "tutorials/python/4-data-cubes.html#summary",
    "title": "Working with xarray data cubes",
    "section": "Summary",
    "text": "Summary\nOnce we have an xarray DataArray, there are many things we can do with it like aggregation, sampling, means and interpolation. See this tutorial for examples. See the xarray gallery in the documentation.\nWe will use the GHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis data from NCEI. It is lower resolution than the MUR data and will load faster. Also keep in mind that we are on a JupyterHub on Azure while the NASA data is on AWS us-west-2. This means we have to do data access via https and not direct access via S3 and our data access is much slower.\n\nImport Required Packages\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\nfrom pprint import pprint\n\nimport earthaccess\nimport xarray as xr",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Monthly and seasonal means"
    ]
  },
  {
    "objectID": "tutorials/python/4-data-cubes.html#create-our-data-cube",
    "href": "tutorials/python/4-data-cubes.html#create-our-data-cube",
    "title": "Working with xarray data cubes",
    "section": "Create our data cube",
    "text": "Create our data cube\n\nAuthenticate\n\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)\n\n\nGet a vector of urls to our nc files\n\n\nshort_name = 'AVHRR_OI-NCEI-L4-GLOB-v2.1'\nversion = \"2.1\"\ndate_range = (\"2020-01-02\", \"2020-12-31\")\n\nresults = earthaccess.search_data(\n    short_name = short_name,\n    version = version,\n    temporal = date_range,\n)\n\nGranules found: 366\n\n\nWe can look at the data links and see that we will be using https.\n\nresults[0].data_links()\n\n['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20191231120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc']\n\n\n\nCreate our data cube and save\n\nThis would be instant if our hub were on AWS us-west-2 with the NASA data, but will take a minute or two since we have to use https.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Monthly and seasonal means"
    ]
  },
  {
    "objectID": "tutorials/python/4-data-cubes.html#means",
    "href": "tutorials/python/4-data-cubes.html#means",
    "title": "Working with xarray data cubes",
    "section": "Means",
    "text": "Means\nOverall mean, all the data.\n\ndc.mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' ()&gt; Size: 4B\narray(298.5631, dtype=float32)xarray.DataArray'analysed_sst'298.6array(298.5631, dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nDaily means over all space.\n\ndc.mean(dim=['lat', 'lon'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 366)&gt; Size: 1kB\narray([296.73938, 296.89343, 296.92328, 296.5633 , 296.70032, 296.6867 ,\n       296.5006 , 296.46686, 296.1819 , 296.1514 , 296.5408 , 296.08966,\n       295.66736, 296.16452, 295.9995 , 295.877  , 295.8689 , 295.81546,\n       295.4739 , 295.47705, 295.5378 , 295.7533 , 296.26517, 296.2378 ,\n       296.21436, 296.1822 , 295.98047, 295.8164 , 295.77423, 295.4317 ,\n       295.04297, 294.88858, 295.22015, 295.47452, 295.70532, 295.77185,\n       295.7725 , 295.91983, 295.9339 , 295.44202, 295.20624, 295.39514,\n       295.65704, 295.5511 , 295.38702, 295.16672, 295.48843, 295.60126,\n       295.6162 , 295.88547, 295.90265, 295.3266 , 294.7489 , 294.76062,\n       295.0053 , 295.41937, 295.71063, 295.4889 , 295.3653 , 295.50092,\n       295.05002, 295.18872, 296.33875, 296.76828, 296.80655, 296.55017,\n       297.15778, 296.6717 , 296.58655, 296.70078, 297.09686, 297.0692 ,\n       297.34595, 297.4648 , 297.54623, 297.45624, 297.36905, 297.41486,\n       297.29703, 296.94843, 297.00296, 297.10297, 297.04312, 296.8697 ,\n       296.79578, 296.73407, 296.51404, 296.18063, 296.0562 , 296.51172,\n       297.10498, 296.7483 , 296.03015, 295.92047, 295.88983, 296.16718,\n       296.38904, 296.59125, 296.5714 , 296.40424, 296.64355, 296.26392,\n       296.03467, 295.97986, 295.99875, 296.44983, 296.52673, 296.5022 ,\n       296.50577, 296.57452, 296.70624, 296.77856, 296.8775 , 296.67688,\n       296.6355 , 297.33484, 297.4214 , 298.06763, 298.31906, 298.20358,\n...\n       302.57956, 302.55872, 302.62   , 302.58328, 302.49185, 302.51434,\n       302.33905, 302.04706, 301.8042 , 301.58594, 301.35706, 301.13138,\n       300.88452, 300.43124, 300.33765, 300.18933, 300.16235, 300.14844,\n       300.14154, 300.1325 , 300.32718, 300.47137, 300.50735, 300.43372,\n       300.39575, 300.45062, 300.4033 , 300.4425 , 300.3039 , 300.35266,\n       300.3156 , 300.26   , 300.0495 , 300.2542 , 300.35547, 300.2417 ,\n       299.96796, 299.90717, 299.70624, 299.72375, 299.85138, 299.90015,\n       299.88782, 300.08936, 300.11   , 300.11578, 300.1589 , 299.89825,\n       299.90952, 299.89752, 299.88358, 299.8698 , 299.54364, 299.56876,\n       299.3811 , 299.2125 , 298.97998, 298.70248, 298.53064, 298.51123,\n       298.48468, 298.60687, 298.55936, 298.48657, 298.63562, 298.5814 ,\n       298.34875, 298.12827, 298.09656, 298.14514, 298.11514, 297.3639 ,\n       297.1286 , 297.10577, 297.1211 , 297.15747, 296.96002, 297.01843,\n       297.25876, 297.4072 , 297.40875, 297.13153, 297.27155, 297.52905,\n       297.53876, 296.89252, 296.70795, 296.85562, 296.985  , 297.0583 ,\n       296.807  , 296.40845, 296.39642, 296.32092, 296.14795, 296.52753,\n       296.67593, 296.8983 , 296.97797, 297.04175, 296.1742 , 296.05078,\n       295.62952, 295.8086 , 295.93842, 295.68674, 295.57547, 295.6925 ,\n       295.84045, 295.65625, 295.6156 , 295.69235, 295.31842, 295.1786 ],\n      dtype=float32)\nCoordinates:\n  * time     (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31xarray.DataArray'analysed_sst'time: 366296.7 296.9 296.9 296.6 296.7 296.7 ... 295.7 295.6 295.7 295.3 295.2array([296.73938, 296.89343, 296.92328, 296.5633 , 296.70032, 296.6867 ,\n       296.5006 , 296.46686, 296.1819 , 296.1514 , 296.5408 , 296.08966,\n       295.66736, 296.16452, 295.9995 , 295.877  , 295.8689 , 295.81546,\n       295.4739 , 295.47705, 295.5378 , 295.7533 , 296.26517, 296.2378 ,\n       296.21436, 296.1822 , 295.98047, 295.8164 , 295.77423, 295.4317 ,\n       295.04297, 294.88858, 295.22015, 295.47452, 295.70532, 295.77185,\n       295.7725 , 295.91983, 295.9339 , 295.44202, 295.20624, 295.39514,\n       295.65704, 295.5511 , 295.38702, 295.16672, 295.48843, 295.60126,\n       295.6162 , 295.88547, 295.90265, 295.3266 , 294.7489 , 294.76062,\n       295.0053 , 295.41937, 295.71063, 295.4889 , 295.3653 , 295.50092,\n       295.05002, 295.18872, 296.33875, 296.76828, 296.80655, 296.55017,\n       297.15778, 296.6717 , 296.58655, 296.70078, 297.09686, 297.0692 ,\n       297.34595, 297.4648 , 297.54623, 297.45624, 297.36905, 297.41486,\n       297.29703, 296.94843, 297.00296, 297.10297, 297.04312, 296.8697 ,\n       296.79578, 296.73407, 296.51404, 296.18063, 296.0562 , 296.51172,\n       297.10498, 296.7483 , 296.03015, 295.92047, 295.88983, 296.16718,\n       296.38904, 296.59125, 296.5714 , 296.40424, 296.64355, 296.26392,\n       296.03467, 295.97986, 295.99875, 296.44983, 296.52673, 296.5022 ,\n       296.50577, 296.57452, 296.70624, 296.77856, 296.8775 , 296.67688,\n       296.6355 , 297.33484, 297.4214 , 298.06763, 298.31906, 298.20358,\n...\n       302.57956, 302.55872, 302.62   , 302.58328, 302.49185, 302.51434,\n       302.33905, 302.04706, 301.8042 , 301.58594, 301.35706, 301.13138,\n       300.88452, 300.43124, 300.33765, 300.18933, 300.16235, 300.14844,\n       300.14154, 300.1325 , 300.32718, 300.47137, 300.50735, 300.43372,\n       300.39575, 300.45062, 300.4033 , 300.4425 , 300.3039 , 300.35266,\n       300.3156 , 300.26   , 300.0495 , 300.2542 , 300.35547, 300.2417 ,\n       299.96796, 299.90717, 299.70624, 299.72375, 299.85138, 299.90015,\n       299.88782, 300.08936, 300.11   , 300.11578, 300.1589 , 299.89825,\n       299.90952, 299.89752, 299.88358, 299.8698 , 299.54364, 299.56876,\n       299.3811 , 299.2125 , 298.97998, 298.70248, 298.53064, 298.51123,\n       298.48468, 298.60687, 298.55936, 298.48657, 298.63562, 298.5814 ,\n       298.34875, 298.12827, 298.09656, 298.14514, 298.11514, 297.3639 ,\n       297.1286 , 297.10577, 297.1211 , 297.15747, 296.96002, 297.01843,\n       297.25876, 297.4072 , 297.40875, 297.13153, 297.27155, 297.52905,\n       297.53876, 296.89252, 296.70795, 296.85562, 296.985  , 297.0583 ,\n       296.807  , 296.40845, 296.39642, 296.32092, 296.14795, 296.52753,\n       296.67593, 296.8983 , 296.97797, 297.04175, 296.1742 , 296.05078,\n       295.62952, 295.8086 , 295.93842, 295.68674, 295.57547, 295.6925 ,\n       295.84045, 295.65625, 295.6156 , 295.69235, 295.31842, 295.1786 ],\n      dtype=float32)Coordinates: (1)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))Attributes: (0)\n\n\nMean over all days.\n\ndc.mean(dim=['time'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (lat: 8, lon: 8)&gt; Size: 256B\narray([[299.256  , 298.8255 , 298.42233, 298.09183, 297.85486, 297.70514,\n        297.63184, 297.61264],\n       [299.5517 , 299.1316 , 298.674  , 298.26437, 297.9496 , 297.75726,\n        297.66498, 297.637  ],\n       [299.7663 , 299.4421 , 298.9925 , 298.51376, 298.12073, 297.86743,\n        297.73553, 297.6641 ],\n       [299.84933, 299.74045, 299.3821 , 298.88126, 298.40146, 298.06958,\n        297.85547, 297.69308],\n       [299.68115, 299.94406, 299.80054, 299.34943, 298.8011 , 298.34732,\n        298.01627, 297.73294],\n       [298.922  , 299.69925, 299.97003, 299.76263, 299.2471 , 298.70444,\n        298.24847, 297.82803],\n       [297.21112, 298.42197, 299.31717, 299.7068 , 299.55844, 299.1261 ,\n        298.60266, 298.04752],\n       [294.91473, 296.2261 , 297.65552, 298.82178, 299.41824, 299.44717,\n        299.0521 , 298.4514 ]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 32B 33.62 33.88 34.12 34.38 34.62 34.88 35.12 35.38\n  * lon      (lon) float32 32B -75.38 -75.12 -74.88 ... -74.12 -73.88 -73.62xarray.DataArray'analysed_sst'lat: 8lon: 8299.3 298.8 298.4 298.1 297.9 297.7 ... 298.8 299.4 299.4 299.1 298.5array([[299.256  , 298.8255 , 298.42233, 298.09183, 297.85486, 297.70514,\n        297.63184, 297.61264],\n       [299.5517 , 299.1316 , 298.674  , 298.26437, 297.9496 , 297.75726,\n        297.66498, 297.637  ],\n       [299.7663 , 299.4421 , 298.9925 , 298.51376, 298.12073, 297.86743,\n        297.73553, 297.6641 ],\n       [299.84933, 299.74045, 299.3821 , 298.88126, 298.40146, 298.06958,\n        297.85547, 297.69308],\n       [299.68115, 299.94406, 299.80054, 299.34943, 298.8011 , 298.34732,\n        298.01627, 297.73294],\n       [298.922  , 299.69925, 299.97003, 299.76263, 299.2471 , 298.70444,\n        298.24847, 297.82803],\n       [297.21112, 298.42197, 299.31717, 299.7068 , 299.55844, 299.1261 ,\n        298.60266, 298.04752],\n       [294.91473, 296.2261 , 297.65552, 298.82178, 299.41824, 299.44717,\n        299.0521 , 298.4514 ]], dtype=float32)Coordinates: (2)lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Indexes: (2)latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Monthly and seasonal means"
    ]
  },
  {
    "objectID": "tutorials/python/4-data-cubes.html#grouped-means",
    "href": "tutorials/python/4-data-cubes.html#grouped-means",
    "title": "Working with xarray data cubes",
    "section": "Grouped Means",
    "text": "Grouped Means\nMonthly, seasonal means, or custom. xarray is designed help you do typical climate calculations. See this tutorial for examples. See the xarray gallery in the documentation. This tutorial shows means with weighting.\n\nMonthly\n\ndc_monthly = dc.resample(time='1MS').mean()\ndc_monthly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'analysed_sst' (time: 12, lat: 8, lon: 8)&gt; Size: 3kB\narray([[[296.91614, 296.48288, 296.22324, 296.16   , 296.1964 ,\n         296.07355, 295.80743, 295.4726 ],\n        [297.16415, 296.80356, 296.51352, 296.33163, 296.19284,\n         295.9984 , 295.68124, 295.29578],\n        [297.13965, 296.9329 , 296.7019 , 296.48032, 296.27747,\n         296.009  , 295.5878 , 295.16193],\n        [297.1003 , 297.0984 , 296.96936, 296.7497 , 296.46744,\n         296.12775, 295.62772, 295.13358],\n        [297.02063, 297.2971 , 297.25   , 297.0139 , 296.66196,\n         296.22934, 295.68643, 295.11255],\n        [296.29773, 297.15997, 297.34677, 297.15097, 296.7677 ,\n         296.30838, 295.80228, 295.21225],\n        [293.87357, 295.57614, 296.5371 , 296.86255, 296.6787 ,\n         296.3432 , 295.90228, 295.32126],\n        [290.24963, 292.47968, 294.46033, 295.85062, 296.49615,\n         296.558  , 296.20642, 295.63773]],\n\n       [[296.3342 , 295.4772 , 294.74622, 294.31934, 294.16925,\n         294.2638 , 294.38898, 294.4645 ],\n        [296.92587, 296.05792, 295.19794, 294.60657, 294.3103 ,\n...\n         298.79996, 298.06934, 297.46368],\n        [294.25702, 295.36368, 296.789  , 298.232  , 299.11603,\n         299.1737 , 298.5793 , 297.94467]],\n\n       [[297.47675, 296.9103 , 296.28128, 295.71838, 295.37872,\n         295.26645, 295.30548, 295.4164 ],\n        [297.6929 , 297.20355, 296.57605, 295.95093, 295.51385,\n         295.3145 , 295.24738, 295.26065],\n        [297.78705, 297.49353, 296.95062, 296.30414, 295.76477,\n         295.4597 , 295.2748 , 295.1558 ],\n        [297.7706 , 297.7881 , 297.4032 , 296.78485, 296.12967,\n         295.7232 , 295.40552, 295.14032],\n        [297.528  , 297.9558 , 297.8684 , 297.33127, 296.60452,\n         296.08032, 295.6409 , 295.22516],\n        [296.62164, 297.469  , 297.82608, 297.66513, 297.12384,\n         296.641  , 296.08096, 295.49902],\n        [294.56384, 295.72772, 296.73026, 297.32483, 297.4303 ,\n         297.33844, 296.7729 , 296.05386],\n        [291.86996, 293.0355 , 294.47284, 295.90158, 296.97806,\n         297.611  , 297.36743, 296.6858 ]]], dtype=float32)\nCoordinates:\n  * lat      (lat) float32 32B 33.62 33.88 34.12 34.38 34.62 34.88 35.12 35.38\n  * lon      (lon) float32 32B -75.38 -75.12 -74.88 ... -74.12 -73.88 -73.62\n  * time     (time) datetime64[ns] 96B 2020-01-01 2020-02-01 ... 2020-12-01\nAttributes:\n    long_name:      analysed sea surface temperature\n    standard_name:  sea_surface_temperature\n    units:          kelvin\n    valid_min:      -300\n    valid_max:      4500\n    source:         UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-...\n    comment:        Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 20...xarray.DataArray'analysed_sst'time: 12lat: 8lon: 8296.9 296.5 296.2 296.2 296.2 296.1 ... 295.9 297.0 297.6 297.4 296.7array([[[296.91614, 296.48288, 296.22324, 296.16   , 296.1964 ,\n         296.07355, 295.80743, 295.4726 ],\n        [297.16415, 296.80356, 296.51352, 296.33163, 296.19284,\n         295.9984 , 295.68124, 295.29578],\n        [297.13965, 296.9329 , 296.7019 , 296.48032, 296.27747,\n         296.009  , 295.5878 , 295.16193],\n        [297.1003 , 297.0984 , 296.96936, 296.7497 , 296.46744,\n         296.12775, 295.62772, 295.13358],\n        [297.02063, 297.2971 , 297.25   , 297.0139 , 296.66196,\n         296.22934, 295.68643, 295.11255],\n        [296.29773, 297.15997, 297.34677, 297.15097, 296.7677 ,\n         296.30838, 295.80228, 295.21225],\n        [293.87357, 295.57614, 296.5371 , 296.86255, 296.6787 ,\n         296.3432 , 295.90228, 295.32126],\n        [290.24963, 292.47968, 294.46033, 295.85062, 296.49615,\n         296.558  , 296.20642, 295.63773]],\n\n       [[296.3342 , 295.4772 , 294.74622, 294.31934, 294.16925,\n         294.2638 , 294.38898, 294.4645 ],\n        [296.92587, 296.05792, 295.19794, 294.60657, 294.3103 ,\n...\n         298.79996, 298.06934, 297.46368],\n        [294.25702, 295.36368, 296.789  , 298.232  , 299.11603,\n         299.1737 , 298.5793 , 297.94467]],\n\n       [[297.47675, 296.9103 , 296.28128, 295.71838, 295.37872,\n         295.26645, 295.30548, 295.4164 ],\n        [297.6929 , 297.20355, 296.57605, 295.95093, 295.51385,\n         295.3145 , 295.24738, 295.26065],\n        [297.78705, 297.49353, 296.95062, 296.30414, 295.76477,\n         295.4597 , 295.2748 , 295.1558 ],\n        [297.7706 , 297.7881 , 297.4032 , 296.78485, 296.12967,\n         295.7232 , 295.40552, 295.14032],\n        [297.528  , 297.9558 , 297.8684 , 297.33127, 296.60452,\n         296.08032, 295.6409 , 295.22516],\n        [296.62164, 297.469  , 297.82608, 297.66513, 297.12384,\n         296.641  , 296.08096, 295.49902],\n        [294.56384, 295.72772, 296.73026, 297.32483, 297.4303 ,\n         297.33844, 296.7729 , 296.05386],\n        [291.86996, 293.0355 , 294.47284, 295.90158, 296.97806,\n         297.611  , 297.36743, 296.6858 ]]], dtype=float32)Coordinates: (3)lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)time(time)datetime64[ns]2020-01-01 ... 2020-12-01long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-02-01T00:00:00.000000000',\n       '2020-03-01T00:00:00.000000000', '2020-04-01T00:00:00.000000000',\n       '2020-05-01T00:00:00.000000000', '2020-06-01T00:00:00.000000000',\n       '2020-07-01T00:00:00.000000000', '2020-08-01T00:00:00.000000000',\n       '2020-09-01T00:00:00.000000000', '2020-10-01T00:00:00.000000000',\n       '2020-11-01T00:00:00.000000000', '2020-12-01T00:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (3)latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01',\n               '2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01',\n               '2020-09-01', '2020-10-01', '2020-11-01', '2020-12-01'],\n              dtype='datetime64[ns]', name='time', freq='MS'))Attributes: (7)long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.\n\n\n\ndc_monthly.plot(x='lon', y='lat', col=\"time\", col_wrap=3);\n\n\n\n\n\n\n\n\n\ndc_monthly.mean(dim=['lat', 'lon']).to_pandas().plot();\n\n\n\n\n\n\n\n\n\n\nSeasonal (quarterly)\n\ndc_qtr = dc.resample(time='Q').mean()\ndc_qtr.mean(dim=['lat', 'lon']).to_pandas().plot()\n\n\n\n\n\n\n\n\n\ndc_qtr.plot(x='lon', y='lat', col=\"time\", col_wrap=2);",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Monthly and seasonal means"
    ]
  },
  {
    "objectID": "tutorials/python/4-data-cubes.html#summary-1",
    "href": "tutorials/python/4-data-cubes.html#summary-1",
    "title": "Working with xarray data cubes",
    "section": "Summary",
    "text": "Summary\nWe learned how to do some simple spatial and temporal means with xarray data cubes.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Monthly and seasonal means"
    ]
  },
  {
    "objectID": "tutorials/python/4-data-cubes.html#references",
    "href": "tutorials/python/4-data-cubes.html#references",
    "title": "Working with xarray data cubes",
    "section": "References",
    "text": "References\n\nAn Introduction to Earth and Environmental Data Science\nxarray user guide",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Monthly and seasonal means"
    ]
  },
  {
    "objectID": "tutorials/python/2-subset-and-plot.html#summary",
    "href": "tutorials/python/2-subset-and-plot.html#summary",
    "title": "Data subsetting and plotting with earthaccess and xarray",
    "section": "Summary",
    "text": "Summary\nIn this examples we will use the xarray and earthaccess to subset data and make figures.\nFor this tutorial we will use the GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1) data. This is much higher resolution data than the AVHRR data and we will do spatially subsetting to a small area of interest.\n\nImport Required Packages\n\n# Suppress warnings\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\nfrom pprint import pprint\n\nimport earthaccess\nimport xarray as xr",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/python/2-subset-and-plot.html#authenticate-to-nasa-earthdata",
    "href": "tutorials/python/2-subset-and-plot.html#authenticate-to-nasa-earthdata",
    "title": "Data subsetting and plotting with earthaccess and xarray",
    "section": "Authenticate to NASA Earthdata",
    "text": "Authenticate to NASA Earthdata\nWe will authenticate our Earthaccess session, and then open the results like we did in the Search & Discovery section.\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/python/2-subset-and-plot.html#get-a-vector-of-urls-to-our-nc-files",
    "href": "tutorials/python/2-subset-and-plot.html#get-a-vector-of-urls-to-our-nc-files",
    "title": "Data subsetting and plotting with earthaccess and xarray",
    "section": "Get a vector of urls to our nc files",
    "text": "Get a vector of urls to our nc files\n\nshort_name = 'MUR-JPL-L4-GLOB-v4.1'\nversion = \"4.1\"\ndate_start = \"2020-01-01\"\ndate_end = \"2020-04-01\"\ndate_range = (date_start, date_end)\n# min lon, min lat, max lon, max lat\nbbox = (-75.5, 33.5, -73.5, 35.5)  \n\nresults = earthaccess.search_data(\n    short_name = short_name,\n    version = version,\n    cloud_hosted = True,\n    temporal = date_range,\n    bounding_box = bbox,\n)\n\nGranules found: 92",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/python/2-subset-and-plot.html#crop-and-plot-one-netcdf-file",
    "href": "tutorials/python/2-subset-and-plot.html#crop-and-plot-one-netcdf-file",
    "title": "Data subsetting and plotting with earthaccess and xarray",
    "section": "Crop and plot one netCDF file",
    "text": "Crop and plot one netCDF file\nEach MUR SST netCDF file is large so I do not want to download. Instead we will subset the data on the server side. We will start with one file.\n\nfileset = earthaccess.open(results[0:1])\nds = xr.open_dataset(fileset[0])\n\nOpening 1 granules, approx size: 0.66 GB\n\n\n\n\n\n\n\n\n\n\n\nNote that xarray works with “lazy” computation whenever possible. In this case, the metadata are loaded into JupyterHub memory, but the data arrays and their values are not — until there is a need for them.\nLet’s print out all the variable names.\n\nfor v in ds.variables:\n    print(v)\n\ntime\nlat\nlon\nanalysed_sst\nanalysis_error\nmask\nsea_ice_fraction\ndt_1km_data\nsst_anomaly\n\n\nOf the variables listed above, we are interested in analysed_sst.\n\nds.variables['analysed_sst'].attrs\n\n{'long_name': 'analysed sea surface temperature',\n 'standard_name': 'sea_surface_foundation_temperature',\n 'units': 'kelvin',\n 'valid_min': -32767,\n 'valid_max': 32767,\n 'comment': '\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolation',\n 'source': 'MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF'}\n\n\n\nSubsetting\nIn addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection/regridding, and file aggregation. What does subsetting mean? To subset means to extract only the portions of a dataset that are needed for a given purpose.\nThere are three primary types of subsetting that we will walk through: 1. Temporal 2. Spatial 3. Variable\nIn each case, we will be excluding parts of the dataset that are not wanted using xarray. Note that “subsetting” is also called a data “transformation”.\n\n# Display the full dataset's metadata\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 29GB\nDimensions:           (time: 1, lat: 17999, lon: 36000)\nCoordinates:\n  * time              (time) datetime64[ns] 8B 2020-01-16T09:00:00\n  * lat               (lat) float32 72kB -89.99 -89.98 -89.97 ... 89.98 89.99\n  * lon               (lon) float32 144kB -180.0 -180.0 -180.0 ... 180.0 180.0\nData variables:\n    analysed_sst      (time, lat, lon) float64 5GB ...\n    analysis_error    (time, lat, lon) float64 5GB ...\n    mask              (time, lat, lon) float32 3GB ...\n    sea_ice_fraction  (time, lat, lon) float64 5GB ...\n    dt_1km_data       (time, lat, lon) timedelta64[ns] 5GB ...\n    sst_anomaly       (time, lat, lon) float64 5GB ...\nAttributes: (47)xarray.DatasetDimensions:time: 1lat: 17999lon: 36000Coordinates: (3)time(time)datetime64[ns]2020-01-16T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2020-01-16T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float64...long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF[647964000 values with dtype=float64]analysis_error(time, lat, lon)float64...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :uncertainty in \"analysed_sst\"[647964000 values with dtype=float64]mask(time, lat, lon)float32...long_name :sea/land field composite maskvalid_min :1valid_max :31flag_masks :[ 1  2  4  8 16]flag_meanings :open_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_gridcomment :mask can be used to further filter the data.source :GMT \"grdlandmask\", ice flag from sea_ice_fraction data[647964000 values with dtype=float32]sea_ice_fraction(time, lat, lon)float64...long_name :sea ice area fractionstandard_name :sea_ice_area_fractionvalid_min :0valid_max :100source :EUMETSAT OSI-SAF, copyright EUMETSATcomment :ice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.[647964000 values with dtype=float64]dt_1km_data(time, lat, lon)timedelta64[ns]...long_name :time to most recent 1km datavalid_min :-127valid_max :127source :MODIS and VIIRS pixels ingested by MURcomment :The grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point.  \"Fill value\" indicates absence of such 1km data at the grid point.[647964000 values with dtype=timedelta64[ns]]sst_anomaly(time, lat, lon)float64...long_name :SST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 periodunits :kelvinvalid_min :-32767valid_max :32767comment :anomaly reference to the day-of-year average between 2003 and 2014[647964000 values with dtype=float64]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-16 09:00:00'], dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([-89.98999786376953,  -89.9800033569336, -89.97000122070312,\n       -89.95999908447266, -89.94999694824219, -89.94000244140625,\n       -89.93000030517578, -89.91999816894531, -89.91000366210938,\n        -89.9000015258789,\n       ...\n         89.9000015258789,  89.91000366210938,  89.91999816894531,\n        89.93000030517578,  89.94000244140625,  89.94999694824219,\n        89.95999908447266,  89.97000122070312,   89.9800033569336,\n        89.98999786376953],\n      dtype='float32', name='lat', length=17999))lonPandasIndexPandasIndex(Index([-179.99000549316406, -179.97999572753906, -179.97000122070312,\n        -179.9600067138672,  -179.9499969482422, -179.94000244140625,\n       -179.92999267578125,  -179.9199981689453, -179.91000366210938,\n       -179.89999389648438,\n       ...\n        179.91000366210938,   179.9199981689453,  179.92999267578125,\n        179.94000244140625,   179.9499969482422,   179.9600067138672,\n        179.97000122070312,  179.97999572753906,  179.99000549316406,\n                     180.0],\n      dtype='float32', name='lon', length=36000))Attributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Resolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20200125T004239Zstart_time :20200116T090000Zstop_time :20200116T090000Ztime_coverage_start :20200115T210000Ztime_coverage_end :20200116T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\nNow we will prepare a subset. We’re using essentially the same spatial bounds as above; however, as opposed to the earthaccess inputs above, here we must provide inputs in the formats expected by xarray. Instead of a single, four-element, bounding box, we use Python slice objects, which are defined by starting and ending numbers.\n\nds_subset = ds.sel(time=date_start, lat=slice(33.5, 35.5), lon=slice(-75.5, -73.5)) \nds_subset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:           (time: 1, lat: 201, lon: 201)\nCoordinates:\n  * time              (time) datetime64[ns] 8B 2020-01-16T09:00:00\n  * lat               (lat) float32 804B 33.5 33.51 33.52 ... 35.48 35.49 35.5\n  * lon               (lon) float32 804B -75.5 -75.49 -75.48 ... -73.51 -73.5\nData variables:\n    analysed_sst      (time, lat, lon) float64 323kB ...\n    analysis_error    (time, lat, lon) float64 323kB ...\n    mask              (time, lat, lon) float32 162kB ...\n    sea_ice_fraction  (time, lat, lon) float64 323kB ...\n    dt_1km_data       (time, lat, lon) timedelta64[ns] 323kB ...\n    sst_anomaly       (time, lat, lon) float64 323kB ...\nAttributes: (47)xarray.DatasetDimensions:time: 1lat: 201lon: 201Coordinates: (3)time(time)datetime64[ns]2020-01-16T09:00:00long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2020-01-16T09:00:00.000000000'], dtype='datetime64[ns]')lat(lat)float3233.5 33.51 33.52 ... 35.49 35.5long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([33.5 , 33.51, 33.52, ..., 35.48, 35.49, 35.5 ], dtype=float32)lon(lon)float32-75.5 -75.49 ... -73.51 -73.5long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-75.5 , -75.49, -75.48, ..., -73.52, -73.51, -73.5 ], dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float64...long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF[40401 values with dtype=float64]analysis_error(time, lat, lon)float64...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :uncertainty in \"analysed_sst\"[40401 values with dtype=float64]mask(time, lat, lon)float32...long_name :sea/land field composite maskvalid_min :1valid_max :31flag_masks :[ 1  2  4  8 16]flag_meanings :open_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_gridcomment :mask can be used to further filter the data.source :GMT \"grdlandmask\", ice flag from sea_ice_fraction data[40401 values with dtype=float32]sea_ice_fraction(time, lat, lon)float64...long_name :sea ice area fractionstandard_name :sea_ice_area_fractionvalid_min :0valid_max :100source :EUMETSAT OSI-SAF, copyright EUMETSATcomment :ice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.[40401 values with dtype=float64]dt_1km_data(time, lat, lon)timedelta64[ns]...long_name :time to most recent 1km datavalid_min :-127valid_max :127source :MODIS and VIIRS pixels ingested by MURcomment :The grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point.  \"Fill value\" indicates absence of such 1km data at the grid point.[40401 values with dtype=timedelta64[ns]]sst_anomaly(time, lat, lon)float64...long_name :SST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 periodunits :kelvinvalid_min :-32767valid_max :32767comment :anomaly reference to the day-of-year average between 2003 and 2014[40401 values with dtype=float64]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-16 09:00:00'], dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([              33.5,   33.5099983215332,  33.52000045776367,\n       33.529998779296875, 33.540000915527344,  33.54999923706055,\n       33.560001373291016,  33.56999969482422,  33.58000183105469,\n        33.59000015258789,\n       ...\n        35.40999984741211,  35.41999816894531,  35.43000030517578,\n       35.439998626708984,  35.45000076293945, 35.459999084472656,\n       35.470001220703125,  35.47999954223633,   35.4900016784668,\n                     35.5],\n      dtype='float32', name='lat', length=201))lonPandasIndexPandasIndex(Index([             -75.5, -75.48999786376953,  -75.4800033569336,\n       -75.47000122070312, -75.45999908447266, -75.44999694824219,\n       -75.44000244140625, -75.43000030517578, -75.41999816894531,\n       -75.41000366210938,\n       ...\n       -73.58999633789062, -73.58000183105469, -73.56999969482422,\n       -73.55999755859375, -73.55000305175781, -73.54000091552734,\n       -73.52999877929688,  -73.5199966430664, -73.51000213623047,\n                    -73.5],\n      dtype='float32', name='lon', length=201))Attributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Resolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20200125T004239Zstart_time :20200116T090000Zstop_time :20200116T090000Ztime_coverage_start :20200115T210000Ztime_coverage_end :20200116T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid\n\n\n\n\nPlotting\nWe will first plot using the methods built-in to the xarray package.\nNote that, as opposed to the “lazy” loading of metadata previously, this will now perform “eager” computation, pulling the required data chunks.\n\nds_subset['analysed_sst'].plot(figsize=(10,6), x='lon', y='lat');",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/python/2-subset-and-plot.html#create-a-data-cube-by-combining-multiple-netcdf-files",
    "href": "tutorials/python/2-subset-and-plot.html#create-a-data-cube-by-combining-multiple-netcdf-files",
    "title": "Data subsetting and plotting with earthaccess and xarray",
    "section": "Create a data cube by combining multiple netCDF files",
    "text": "Create a data cube by combining multiple netCDF files\nWhen we open multiple files, we use open_mfdataset(). Once again, we are doing lazy loading. Note this method works best if you are in the same Amazon Web Services (AWS) region as the data (us-west-2) and can use S3 connection. For the EDM workshop, we are on an Azure JupyterHub and are using https connection so this is much much slower. If we had spun up this JupyterHub on AWS us-west-2 where the NASA data are hosted, we could load a whole year of data instantly.\n\nfileset = earthaccess.open(results)\nds = xr.open_mfdataset(fileset)\n\nOpening 92 granules, approx size: 62.96 GB\n\n\n\n\n\n\n\n\n\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 3TB\nDimensions:           (time: 92, lat: 17999, lon: 36000)\nCoordinates:\n  * time              (time) datetime64[ns] 736B 2020-01-01T09:00:00 ... 2020...\n  * lat               (lat) float32 72kB -89.99 -89.98 -89.97 ... 89.98 89.99\n  * lon               (lon) float32 144kB -180.0 -180.0 -180.0 ... 180.0 180.0\nData variables:\n    analysed_sst      (time, lat, lon) float64 477GB dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    analysis_error    (time, lat, lon) float64 477GB dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n    mask              (time, lat, lon) float32 238GB dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sea_ice_fraction  (time, lat, lon) float64 477GB dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    dt_1km_data       (time, lat, lon) timedelta64[ns] 477GB dask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n    sst_anomaly       (time, lat, lon) float64 477GB dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:                CF-1.7\n    title:                      Daily MUR SST, Final product\n    summary:                    A merged, multi-sensor L4 Foundation SST anal...\n    references:                 http://podaac.jpl.nasa.gov/Multi-scale_Ultra-...\n    institution:                Jet Propulsion Laboratory\n    history:                    created at nominal 4-day latency; replaced nr...\n    ...                         ...\n    project:                    NASA Making Earth Science Data Records for Us...\n    publisher_name:             GHRSST Project Office\n    publisher_url:              http://www.ghrsst.org\n    publisher_email:            ghrsst-po@nceo.ac.uk\n    processing_level:           L4\n    cdm_data_type:              gridxarray.DatasetDimensions:time: 92lat: 17999lon: 36000Coordinates: (3)time(time)datetime64[ns]2020-01-01T09:00:00 ... 2020-04-...long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time of analyzed fieldsarray(['2020-01-01T09:00:00.000000000', '2020-01-02T09:00:00.000000000',\n       '2020-01-03T09:00:00.000000000', '2020-01-04T09:00:00.000000000',\n       '2020-01-05T09:00:00.000000000', '2020-01-06T09:00:00.000000000',\n       '2020-01-07T09:00:00.000000000', '2020-01-08T09:00:00.000000000',\n       '2020-01-09T09:00:00.000000000', '2020-01-10T09:00:00.000000000',\n       '2020-01-11T09:00:00.000000000', '2020-01-12T09:00:00.000000000',\n       '2020-01-13T09:00:00.000000000', '2020-01-14T09:00:00.000000000',\n       '2020-01-15T09:00:00.000000000', '2020-01-16T09:00:00.000000000',\n       '2020-01-17T09:00:00.000000000', '2020-01-18T09:00:00.000000000',\n       '2020-01-19T09:00:00.000000000', '2020-01-20T09:00:00.000000000',\n       '2020-01-21T09:00:00.000000000', '2020-01-22T09:00:00.000000000',\n       '2020-01-23T09:00:00.000000000', '2020-01-24T09:00:00.000000000',\n       '2020-01-25T09:00:00.000000000', '2020-01-26T09:00:00.000000000',\n       '2020-01-27T09:00:00.000000000', '2020-01-28T09:00:00.000000000',\n       '2020-01-29T09:00:00.000000000', '2020-01-30T09:00:00.000000000',\n       '2020-01-31T09:00:00.000000000', '2020-02-01T09:00:00.000000000',\n       '2020-02-02T09:00:00.000000000', '2020-02-03T09:00:00.000000000',\n       '2020-02-04T09:00:00.000000000', '2020-02-05T09:00:00.000000000',\n       '2020-02-06T09:00:00.000000000', '2020-02-07T09:00:00.000000000',\n       '2020-02-08T09:00:00.000000000', '2020-02-09T09:00:00.000000000',\n       '2020-02-10T09:00:00.000000000', '2020-02-11T09:00:00.000000000',\n       '2020-02-12T09:00:00.000000000', '2020-02-13T09:00:00.000000000',\n       '2020-02-14T09:00:00.000000000', '2020-02-15T09:00:00.000000000',\n       '2020-02-16T09:00:00.000000000', '2020-02-17T09:00:00.000000000',\n       '2020-02-18T09:00:00.000000000', '2020-02-19T09:00:00.000000000',\n       '2020-02-20T09:00:00.000000000', '2020-02-21T09:00:00.000000000',\n       '2020-02-22T09:00:00.000000000', '2020-02-23T09:00:00.000000000',\n       '2020-02-24T09:00:00.000000000', '2020-02-25T09:00:00.000000000',\n       '2020-02-26T09:00:00.000000000', '2020-02-27T09:00:00.000000000',\n       '2020-02-28T09:00:00.000000000', '2020-02-29T09:00:00.000000000',\n       '2020-03-01T09:00:00.000000000', '2020-03-02T09:00:00.000000000',\n       '2020-03-03T09:00:00.000000000', '2020-03-04T09:00:00.000000000',\n       '2020-03-05T09:00:00.000000000', '2020-03-06T09:00:00.000000000',\n       '2020-03-07T09:00:00.000000000', '2020-03-08T09:00:00.000000000',\n       '2020-03-09T09:00:00.000000000', '2020-03-10T09:00:00.000000000',\n       '2020-03-11T09:00:00.000000000', '2020-03-12T09:00:00.000000000',\n       '2020-03-13T09:00:00.000000000', '2020-03-14T09:00:00.000000000',\n       '2020-03-15T09:00:00.000000000', '2020-03-16T09:00:00.000000000',\n       '2020-03-17T09:00:00.000000000', '2020-03-18T09:00:00.000000000',\n       '2020-03-19T09:00:00.000000000', '2020-03-20T09:00:00.000000000',\n       '2020-03-21T09:00:00.000000000', '2020-03-22T09:00:00.000000000',\n       '2020-03-23T09:00:00.000000000', '2020-03-24T09:00:00.000000000',\n       '2020-03-25T09:00:00.000000000', '2020-03-26T09:00:00.000000000',\n       '2020-03-27T09:00:00.000000000', '2020-03-28T09:00:00.000000000',\n       '2020-03-29T09:00:00.000000000', '2020-03-30T09:00:00.000000000',\n       '2020-03-31T09:00:00.000000000', '2020-04-01T09:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float32-89.99 -89.98 ... 89.98 89.99long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geolocations inherited from the input data without correctionarray([-89.99, -89.98, -89.97, ...,  89.97,  89.98,  89.99], dtype=float32)lon(lon)float32-180.0 -180.0 ... 180.0 180.0long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geolocations inherited from the input data without correctionarray([-179.99, -179.98, -179.97, ...,  179.98,  179.99,  180.  ],\n      dtype=float32)Data variables: (6)analysed_sst(time, lat, lon)float64dask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;long_name :analysed sea surface temperaturestandard_name :sea_surface_foundation_temperatureunits :kelvinvalid_min :-32767valid_max :32767comment :\"Final\" version using Multi-Resolution Variational Analysis (MRVA) method for interpolationsource :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAF\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n444.15 GiB\n15.98 MiB\n\n\nShape\n(92, 17999, 36000)\n(1, 1023, 2047)\n\n\nDask graph\n29808 chunks in 185 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 92\n\n\n\n\n\n\n\n\nanalysis_error\n\n\n(time, lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nestimated error standard deviation of analysed_sst\n\nunits :\n\nkelvin\n\nvalid_min :\n\n0\n\nvalid_max :\n\n32767\n\ncomment :\n\nuncertainty in \"analysed_sst\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n444.15 GiB\n15.98 MiB\n\n\nShape\n(92, 17999, 36000)\n(1, 1023, 2047)\n\n\nDask graph\n29808 chunks in 185 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 92\n\n\n\n\n\n\n\n\nmask\n\n\n(time, lat, lon)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nsea/land field composite mask\n\nvalid_min :\n\n1\n\nvalid_max :\n\n31\n\nflag_masks :\n\n[ 1 2 4 8 16]\n\nflag_meanings :\n\nopen_sea land open_lake open_sea_with_ice_in_the_grid open_lake_with_ice_in_the_grid\n\ncomment :\n\nmask can be used to further filter the data.\n\nsource :\n\nGMT \"grdlandmask\", ice flag from sea_ice_fraction data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n222.07 GiB\n15.98 MiB\n\n\nShape\n(92, 17999, 36000)\n(1, 1447, 2895)\n\n\nDask graph\n15548 chunks in 185 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                                             36000 17999 92\n\n\n\n\n\n\n\n\nsea_ice_fraction\n\n\n(time, lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nsea ice area fraction\n\nstandard_name :\n\nsea_ice_area_fraction\n\nvalid_min :\n\n0\n\nvalid_max :\n\n100\n\nsource :\n\nEUMETSAT OSI-SAF, copyright EUMETSAT\n\ncomment :\n\nice fraction is a dimensionless quantity between 0 and 1; it has been interpolated by a nearest neighbor approach.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n444.15 GiB\n31.96 MiB\n\n\nShape\n(92, 17999, 36000)\n(1, 1447, 2895)\n\n\nDask graph\n15548 chunks in 185 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                             36000 17999 92\n\n\n\n\n\n\n\n\ndt_1km_data\n\n\n(time, lat, lon)\n\n\ntimedelta64[ns]\n\n\ndask.array&lt;chunksize=(1, 1447, 2895), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\ntime to most recent 1km data\n\nvalid_min :\n\n-127\n\nvalid_max :\n\n127\n\nsource :\n\nMODIS and VIIRS pixels ingested by MUR\n\ncomment :\n\nThe grid value is hours between the analysis time and the most recent MODIS or VIIRS 1km L2P datum within 0.01 degrees from the grid point. \"Fill value\" indicates absence of such 1km data at the grid point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n444.15 GiB\n31.96 MiB\n\n\nShape\n(92, 17999, 36000)\n(1, 1447, 2895)\n\n\nDask graph\n15548 chunks in 185 graph layers\n\n\nData type\ntimedelta64[ns] numpy.ndarray\n\n\n\n\n                                                                                                             36000 17999 92\n\n\n\n\n\n\n\n\nsst_anomaly\n\n\n(time, lat, lon)\n\n\nfloat64\n\n\ndask.array&lt;chunksize=(1, 1023, 2047), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlong_name :\n\nSST anomaly from a seasonal SST climatology based on the MUR data over 2003-2014 period\n\nunits :\n\nkelvin\n\nvalid_min :\n\n-32767\n\nvalid_max :\n\n32767\n\ncomment :\n\nanomaly reference to the day-of-year average between 2003 and 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n444.15 GiB\n15.98 MiB\n\n\nShape\n(92, 17999, 36000)\n(1, 1023, 2047)\n\n\nDask graph\n29808 chunks in 185 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                                                                                                                                 36000 17999 92\n\n\n\n\n\nIndexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01 09:00:00', '2020-01-02 09:00:00',\n               '2020-01-03 09:00:00', '2020-01-04 09:00:00',\n               '2020-01-05 09:00:00', '2020-01-06 09:00:00',\n               '2020-01-07 09:00:00', '2020-01-08 09:00:00',\n               '2020-01-09 09:00:00', '2020-01-10 09:00:00',\n               '2020-01-11 09:00:00', '2020-01-12 09:00:00',\n               '2020-01-13 09:00:00', '2020-01-14 09:00:00',\n               '2020-01-15 09:00:00', '2020-01-16 09:00:00',\n               '2020-01-17 09:00:00', '2020-01-18 09:00:00',\n               '2020-01-19 09:00:00', '2020-01-20 09:00:00',\n               '2020-01-21 09:00:00', '2020-01-22 09:00:00',\n               '2020-01-23 09:00:00', '2020-01-24 09:00:00',\n               '2020-01-25 09:00:00', '2020-01-26 09:00:00',\n               '2020-01-27 09:00:00', '2020-01-28 09:00:00',\n               '2020-01-29 09:00:00', '2020-01-30 09:00:00',\n               '2020-01-31 09:00:00', '2020-02-01 09:00:00',\n               '2020-02-02 09:00:00', '2020-02-03 09:00:00',\n               '2020-02-04 09:00:00', '2020-02-05 09:00:00',\n               '2020-02-06 09:00:00', '2020-02-07 09:00:00',\n               '2020-02-08 09:00:00', '2020-02-09 09:00:00',\n               '2020-02-10 09:00:00', '2020-02-11 09:00:00',\n               '2020-02-12 09:00:00', '2020-02-13 09:00:00',\n               '2020-02-14 09:00:00', '2020-02-15 09:00:00',\n               '2020-02-16 09:00:00', '2020-02-17 09:00:00',\n               '2020-02-18 09:00:00', '2020-02-19 09:00:00',\n               '2020-02-20 09:00:00', '2020-02-21 09:00:00',\n               '2020-02-22 09:00:00', '2020-02-23 09:00:00',\n               '2020-02-24 09:00:00', '2020-02-25 09:00:00',\n               '2020-02-26 09:00:00', '2020-02-27 09:00:00',\n               '2020-02-28 09:00:00', '2020-02-29 09:00:00',\n               '2020-03-01 09:00:00', '2020-03-02 09:00:00',\n               '2020-03-03 09:00:00', '2020-03-04 09:00:00',\n               '2020-03-05 09:00:00', '2020-03-06 09:00:00',\n               '2020-03-07 09:00:00', '2020-03-08 09:00:00',\n               '2020-03-09 09:00:00', '2020-03-10 09:00:00',\n               '2020-03-11 09:00:00', '2020-03-12 09:00:00',\n               '2020-03-13 09:00:00', '2020-03-14 09:00:00',\n               '2020-03-15 09:00:00', '2020-03-16 09:00:00',\n               '2020-03-17 09:00:00', '2020-03-18 09:00:00',\n               '2020-03-19 09:00:00', '2020-03-20 09:00:00',\n               '2020-03-21 09:00:00', '2020-03-22 09:00:00',\n               '2020-03-23 09:00:00', '2020-03-24 09:00:00',\n               '2020-03-25 09:00:00', '2020-03-26 09:00:00',\n               '2020-03-27 09:00:00', '2020-03-28 09:00:00',\n               '2020-03-29 09:00:00', '2020-03-30 09:00:00',\n               '2020-03-31 09:00:00', '2020-04-01 09:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([-89.98999786376953,  -89.9800033569336, -89.97000122070312,\n       -89.95999908447266, -89.94999694824219, -89.94000244140625,\n       -89.93000030517578, -89.91999816894531, -89.91000366210938,\n        -89.9000015258789,\n       ...\n         89.9000015258789,  89.91000366210938,  89.91999816894531,\n        89.93000030517578,  89.94000244140625,  89.94999694824219,\n        89.95999908447266,  89.97000122070312,   89.9800033569336,\n        89.98999786376953],\n      dtype='float32', name='lat', length=17999))lonPandasIndexPandasIndex(Index([-179.99000549316406, -179.97999572753906, -179.97000122070312,\n        -179.9600067138672,  -179.9499969482422, -179.94000244140625,\n       -179.92999267578125,  -179.9199981689453, -179.91000366210938,\n       -179.89999389648438,\n       ...\n        179.91000366210938,   179.9199981689453,  179.92999267578125,\n        179.94000244140625,   179.9499969482422,   179.9600067138672,\n        179.97000122070312,  179.97999572753906,  179.99000549316406,\n                     180.0],\n      dtype='float32', name='lon', length=36000))Attributes: (47)Conventions :CF-1.7title :Daily MUR SST, Final productsummary :A merged, multi-sensor L4 Foundation SST analysis product from JPL.references :http://podaac.jpl.nasa.gov/Multi-scale_Ultra-high_Resolution_MUR-SSTinstitution :Jet Propulsion Laboratoryhistory :created at nominal 4-day latency; replaced nrt (1-day latency) version.comment :MUR = \"Multi-scale Ultra-high Resolution\"license :These data are available free of charge under data policy of JPL PO.DAAC.id :MUR-JPL-L4-GLOB-v04.1naming_authority :org.ghrsstproduct_version :04.1uuid :27665bc0-d5fc-11e1-9b23-0800200c9a66gds_version_id :2.0netcdf_version_id :4.1date_created :20200124T180027Zstart_time :20200101T090000Zstop_time :20200101T090000Ztime_coverage_start :20191231T210000Ztime_coverage_end :20200101T210000Zfile_quality_level :3source :MODIS_T-JPL, MODIS_A-JPL, AMSR2-REMSS, AVHRRMTA_G-NAVO, AVHRRMTB_G-NAVO, iQUAM-NOAA/NESDIS, Ice_Conc-OSISAFplatform :Terra, Aqua, GCOM-W, MetOp-A, MetOp-B, Buoys/Shipssensor :MODIS, AMSR2, AVHRR, in-situMetadata_Conventions :Unidata Observation Dataset v1.0metadata_link :http://podaac.jpl.nasa.gov/ws/metadata/dataset/?format=iso&shortName=MUR-JPL-L4-GLOB-v04.1keywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsouthernmost_latitude :-90.0northernmost_latitude :90.0westernmost_longitude :-180.0easternmost_longitude :180.0spatial_resolution :0.01 degreesgeospatial_lat_units :degrees northgeospatial_lat_resolution :0.01geospatial_lon_units :degrees eastgeospatial_lon_resolution :0.01acknowledgment :Please acknowledge the use of these data with the following statement:  These data were provided by JPL under support by NASA MEaSUREs program.creator_name :JPL MUR SST projectcreator_email :ghrsst@podaac.jpl.nasa.govcreator_url :http://mur.jpl.nasa.govproject :NASA Making Earth Science Data Records for Use in Research Environments (MEaSUREs) Programpublisher_name :GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L4cdm_data_type :grid",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "tutorials/python/2-subset-and-plot.html#summary-1",
    "href": "tutorials/python/2-subset-and-plot.html#summary-1",
    "title": "Data subsetting and plotting with earthaccess and xarray",
    "section": "Summary",
    "text": "Summary\nWe learned how to subset xarray data cubes by time and space using sel() and slice(). Next we will show how to select via a shapefile. If you want to jump instead to creating monthly and seasonal means from a data cube, you can look at the 4-data-cube.ipynb tutorial or explore the gallery of xarray examples.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Subset and Crop"
    ]
  },
  {
    "objectID": "team.html#organizers-and-instructors",
    "href": "team.html#organizers-and-instructors",
    "title": "Our Team",
    "section": "Organizers and Instructors",
    "text": "Organizers and Instructors\n\n\nEli Holmes\n\n\nNOAA Fisheries\nwebpage • GitHub • ORCID\n\n\nSunny Hospital\n\nNOAA CoastWatch PolarWatch\nGitHub\n\n\nMatt Grossi\n\nNOAA Fisheries\nwebpage • GitHub • ORCID\n\n\nEmily Markowitz\n\nNOAA Fisheries\nwebpage • GitHub • NOAA • ORCID\n\n\n\n\nSongzhi Liu\n\nNOAA CoastWatch Great Lakes Node\nwebpage • GitHub\n\n\nDan Pendleton\n\nNOAA Fisheries\nwebpage • GitHub • ORCID\n\n\nMore\n\n\nMore",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Our Team"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set-up",
    "section": "",
    "text": "We will be using a JupyterHub for the workshop.\nYou can also see setup - R and setup - Python to aset up on your computer but be aware that GDAL installation is problemmatic.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Set-up"
    ]
  },
  {
    "objectID": "setup.html#access-to-the-jupyterhub",
    "href": "setup.html#access-to-the-jupyterhub",
    "title": "Set-up",
    "section": "Access to the JupyterHub",
    "text": "Access to the JupyterHub\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com. Advice for choosing a GitHub username: this is a professional username that you will use in work settings. GitHub accounts are not anonymous; this is for sharing work. Using your real name is common.\nWrite down your username and password; you will need to log in during the course!\nHere is a video showing the whole process\n\n\n\nGet on JupyterHub\nOnce you have submitted your GitHub username and have been accepted as a member of the DaskHub team on the nmfs-opensci organization, you can log-into the JupyterHub.\nhttps://dhub.opensci.live/\n\nChoose the default Py-R base geospatial image. Watch a video of the login process and basic JupyterHub orientation.\nhome directory is yours and no one else can see it. To share files, you can connect to a GitHub repository or use the shared directory. Everyone can read and write to this directory. Please don’t delete content that is not your own.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Set-up"
    ]
  },
  {
    "objectID": "setup.html#earthdata-login-account-optional",
    "href": "setup.html#earthdata-login-account-optional",
    "title": "Set-up",
    "section": "Earthdata Login account (optional)",
    "text": "Earthdata Login account (optional)\nWe will be using a public user account, but if you do a lot of work with NASA Earthdata, you should get a login account.\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov\nWrite down your username and password; you will need it.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Set-up"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDMW 2024 - Workshop 3B",
    "section": "",
    "text": "Register for the EDM Workshop: Go to website with password EnterpriseData2024 Once registered you can navigate to Workshop 3B Part I and Part II and add them to your schedule.\nWelcome to the NOAA Fisheries workshop focused on geospatial analysis using ocean ‘big data’. Today, we are focused on using data from NASA EarthData but the skills you will learn are transferable to other ways that you might get earth data (e.g., NESDIS, NCEI, ERDDAP servers, Copernicus).\nThis workshop is focused on those who are brand new to working with earth data in the cloud and with geospatial packages. You will be introduced to cloud access to earth data via Python (first hour) and R (second hour). This workshop will also introduce working with JupyterHubs. We will Jupyter Lab (Python) and RStudio (R) within our JupyterHub.\nSCHEDULE (see nav bar to left)",
    "crumbs": [
      "JupyterHub",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "EDMW 2024 - Workshop 3B",
    "section": "Resources",
    "text": "Resources\n\nCoastWatch GitHub organization for many more training modules for working with satellite data in Python and R\nNASA EarthData Cloudbook for many tutorials on using satellite data in Python and R and NASA Earth Data\nNMFS Friday HackHours",
    "crumbs": [
      "JupyterHub",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#thank-you-for-inspiration-and-content",
    "href": "index.html#thank-you-for-inspiration-and-content",
    "title": "EDMW 2024 - Workshop 3B",
    "section": "Thank you for inspiration and content!",
    "text": "Thank you for inspiration and content!\nThank you to the open science community that has created software, teaching resources, and workflows that we have been able to build off of and be inspired by. These include: NASA Openscapes • OceanHackWeek • SnowEx Hackweek • eScience Institute, University of Washington • ICESat-2 Hackweek • Project Jupyter • Pangeo Project • CryoCloud",
    "crumbs": [
      "JupyterHub",
      "Welcome"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "href": "content/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "title": "RStudio - R",
    "section": "Open RStudio in the JupyterHub",
    "text": "Open RStudio in the JupyterHub\n\nLogin the JupyterHub\nClick on the RStudio button when the Launcher appears \nLook for the browser tab with the RStudio icon",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#basic-navigation",
    "href": "content/02-rstudio.html#basic-navigation",
    "title": "RStudio - R",
    "section": "Basic Navigation",
    "text": "Basic Navigation\n\n\n\nRStudio Panels",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#create-an-rstudio-project",
    "href": "content/02-rstudio.html#create-an-rstudio-project",
    "title": "RStudio - R",
    "section": "Create an RStudio project",
    "text": "Create an RStudio project\n\nOpen RStudio\nIn the file panel, click on the Home icon to make sure you are in your home directory\nFrom the file panel, click “New Project” to create a new project\nIn the pop up, select New Directory and then New Project\nName it sandbox\nClick on the dropdown in the upper right corner to select your sandbox project\nClick on Tools &gt; Project Options &gt; General and change the first 2 options about saving and restoring the workspace to “No”",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#installing-packages",
    "href": "content/02-rstudio.html#installing-packages",
    "title": "RStudio - R",
    "section": "Installing packages",
    "text": "Installing packages\nIn the bottom right panel, select the Packages tab, click install and then start typing the name of the package. Then click Install.\nThe JupyterHub comes with many packages already installed so you shouldn’t have to install many packages.\nWhen you want to use a package, you first need to load it with\nlibrary(hello)\nYou will see this in the tutorials. You might also see something like\nhello::thefunction()\nThis is using thefunction() from the hello package.\n\n\n\n\n\n\nNote\n\n\n\nPython users. In R, you will always call a function like funtion(object) and never like object.function(). The exception is something called ‘piping’ in R, which I have never seen in Python. In this case you pass objects left to right. Like object %&gt;% function(). Piping is very common in modern R but you won’t see it much in R from 10 years ago.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#uploading-and-downloading-files",
    "href": "content/02-rstudio.html#uploading-and-downloading-files",
    "title": "RStudio - R",
    "section": "Uploading and downloading files",
    "text": "Uploading and downloading files\nNote, Upload and download is only for the JupyterHub not on RStudio on your computer.\n\nUploading is easy.\nLook for the Upload button in the Files tab of the bottom right panel.\n\n\nDownload is less intuitive.\n\nClick the checkbox next to the file you want to download. One only.\nClick the “cog” icon in the Files tab of the bottom right panel. Then click Export.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#creating-files",
    "href": "content/02-rstudio.html#creating-files",
    "title": "RStudio - R",
    "section": "Creating files",
    "text": "Creating files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/rstudio. Everyone has the same home directory but your files are separate and cannot be seen by others.\nPython users: If you open a Python image instead of the R image, your home is home/jovyan.\nThere are a number of different ways to create new files. Let’s practice making new files in RStudio.\n\nR Script\n\nOpen RStudio\nIn the upper right, make sure you are in your sandbox project.\nFrom the file panel, click on “New Blank File” and create a new R script.\nPaste\n\nprint(\"Hello World\")\n1+1\nin the script. 7. Click the Source button (upper left of your new script file) to run this code. 8. Try putting your cursor on one line and running that line of code by clicking “Run” 9. Try selecting lines of code and running that by clicking “Run”\n\n\ncsv file\n\nFrom the file panel, click on “New Blank File” and create a Text File.\nThe file will open in the top left corner. Paste in the following:\n\nname, place, value\nA, 1, 2\nB, 10, 20\nC, 100, 200\n\nClick the save icon (above your new file) to save your csv file\n\n\n\nA Rmarkdown document\nNow let’s create some more complicated files using the RStudio template feature.\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\nReference sheet for writing in RMarkdown or go to Help &gt; Markdown Quick Reference\n\n\nA Rmarkdown presentation\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Presentation” on left of the popup and click “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\n\n\n(advanced) An interactive application\n\nFrom the upper left, click File -&gt; New File -&gt; Shiny Web App\nIn the popup, give the app a name and make sure the app is saved to my-files\nWhen the file opens, Run App (icon at top of file).\n\n\n\nAnd many more\nPlay around with creating other types of documents using templates. Especially if you already use RStudio.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#more-tips",
    "href": "content/02-rstudio.html#more-tips",
    "title": "RStudio - R",
    "section": "More tips",
    "text": "More tips\nLearn some tips and tricks from these\n\nhttps://colorado.posit.co/rsc/the-unknown/into-the-unknown.html\nhttps://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-rstudio.html#plotting-a-netcdf-file",
    "href": "content/02-rstudio.html#plotting-a-netcdf-file",
    "title": "RStudio - R",
    "section": "Plotting a netCDF file",
    "text": "Plotting a netCDF file\n\nhttps://pjbartlein.github.io/REarthSysSci/netCDF.html\nhttps://r-spatial.github.io/sf/articles/sf1.html\n\nwebpage:\nhttps://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.graph?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.8):(44.8)%5D%5B(39.7):(92.3)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\nurl from the dropdown on that page\nurl &lt;- https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.nc?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.875):(44.875)%5D%5B(39.625):(92.375)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\n\nOpen an R script\n\nAdd this code.\n\nlibrary(ggplot2) # package for plotting\nlibrary(sf)\nlibrary(stars)\nlibrary(dplyr)\n\nurl &lt;- \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg.nc?sst%5B(2023-08-27T12:00:00Z)%5D%5B(0.0)%5D%5B(-7.875):(44.875)%5D%5B(39.625):(92.375)%5D&.draw=surface&.vars=longitude%7Clatitude%7Csst&.colorBar=%7C%7C%7C%7C%7C&.bgColor=0xffccccff\"\n\nfil &lt;- \"sst.nc\"\nif(!exists(fil)){\n  download.file(url=url, destfile=fil)\n}\n\nstars_object &lt;- raster::raster(fil) %&gt;% st_as_stars()\nggplot() + geom_stars(data = stars_object)",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#what-is-git-and-github",
    "href": "content/02-git-rstudio.html#what-is-git-and-github",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "What is Git and GitHub?",
    "text": "What is Git and GitHub?\nGit A program to track your file changes and create a history of those changes. Creates a ‘container’ for a set of files called a repository.\nGitHub A website to host these repositories and allow you to sync local copies (on your computer) to the website. Lots of functionality built on top of this.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#some-basic-git-jargon",
    "href": "content/02-git-rstudio.html#some-basic-git-jargon",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Some basic Git jargon",
    "text": "Some basic Git jargon\n\nRepo Repository. It is your code and the record of your changes. This record and also the status of your repo is a hidden folder called .git . You have a local repo and a remote repo. The remote repo is on GitHub (for in our case) is called origin. The local repo is on the JupyterHub.\nStage Tell Git which changes you want to commit (write to the repo history).\nCommit Write a note about what change the staged files and “commit” that note to the repository record. You are also tagging this state of the repo and you could go back to this state if you wanted.\nPush Push local changes (commits) up to the remote repository on GitHub (origin).\nPull Pull changes on GitHub into the local repository on the JupyterHub.\nGit GUIs A graphical interface for Git (which is command line). Today I will use jupyterlab-git which we have installed on JupyterHub.\nShell A terminal window where we can issue git commands.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#overview",
    "href": "content/02-git-rstudio.html#overview",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Overview",
    "text": "Overview\nToday I will cover the four basic Git/GitHub skills. The goal for today is to first get you comfortable with the basic skills and terminology. We will use what is called a “trunk-based workflow”.\n\nSimple Trunk-based Workflow:\n\nMake local (on your computer) changes to code.\nRecord what those changes were about and commit to the code change record (history).\nPush those changes to your remote repository (aka origin)\n\nWe’ll do this",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#setting-up-git",
    "href": "content/02-git-rstudio.html#setting-up-git",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Setting up Git",
    "text": "Setting up Git\nYou should have gotten this done on Tuesday but if not here are the instructions\nBefore we can work with Git in the JupyterHub, we need to do some set up.\n\nTell Git who you are and to store your credentials (GitHub login info)\n\nShow me\nPaste this into a terminal window:\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\n\nGet a Personal Access Token from GitHub\n\nCopy the token! You will need it in the next step.\nShow me Note, one change to this video is that you need to specify that you want a classic token.\n\nTrigger Git to ask for your password (that personal access token)\n\nYou can do this by cloning a private repo. In the Terminal, issue this command\ngit clone https://github.com/nmfs-opensci/github_setup_check\nIt will ask for your GitHub username and password. At the password part, paste in the Personal Access Token.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#git-tab",
    "href": "content/02-git-rstudio.html#git-tab",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Git tab",
    "text": "Git tab\nWhen the instructions say to use or open or click the Git tab,",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#the-key-skills",
    "href": "content/02-git-rstudio.html#the-key-skills",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo to RStudio\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#lets-see-it-done",
    "href": "content/02-git-rstudio.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\nShow me\n\n\nSkill 2: Clone your repo to the RStudio\nIn RStudio we do this by making a new project.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nFile &gt; New Project &gt; Version Control &gt; Git\nPast in the URL of your repo from Step 1\nCheck that it is being created in your Home directory which will be denoted ~ in the JupyterHub.\nClick Create.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nMake some changes to the README.md file in the Test repo.\nClick the Git tab, and stage the change(s) by checking the checkboxes next to the files listed.\nClick the Commit button.\nAdd a commit comment, click commit.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git tab, click on the Green up arrow that says Push.\nTo pull changes on GitHub that are not on your local computer:\nMake some changes directly on GitHub (not in RStudio)\nFrom Git tab, click on the down arrow that says Pull.\n\nShow me\n\n\nPair-activity 1\nIn RStudio,\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nTry before watching.\nShow me in RStudio – Show me in the shell – Show me in jupyter-git\n\n\nPair-activity 2\nAll of this activity is in RStudio.\n\nClone this repo https://github.com/nmfs-opensci/git-basics to RStudio and create a new project\nNavigate to the files in your new project, create a filed called to &lt;yourname&gt;.md. Use your actual name so the filename is different from everyone elses.\nStage and then commit that new file.\nPush to GitHub.\nMake some more changes and push to GitHub.\nPull in your partner’s (and everyone elses) changes\n\nShow me in RStudio – Show me in JupyterLab\n\n\nPair-activity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to RStudio and create a new project",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-git-rstudio.html#footnotes",
    "href": "content/02-git-rstudio.html#footnotes",
    "title": "Basic Git/GitHub Skills Using RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-RStudio"
    ]
  },
  {
    "objectID": "content/02-earthdata.html#overview",
    "href": "content/02-earthdata.html#overview",
    "title": "Earthdata Login",
    "section": "Overview",
    "text": "Overview",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Earthdata login"
    ]
  },
  {
    "objectID": "content/02-earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "content/02-earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Why do I need an Earthdata login?",
    "text": "Why do I need an Earthdata login?\nWe will be teaching you ways to programmatically access NASA remote-sensing data from within your scripts. You will need to enter your Earthdata username and password in order for this to work.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Earthdata login"
    ]
  },
  {
    "objectID": "content/02-earthdata.html#getting-an-earthdata-login",
    "href": "content/02-earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Getting an Earthdata login",
    "text": "Getting an Earthdata login\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Earthdata login"
    ]
  },
  {
    "objectID": "content/02-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "content/02-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "Configure programmatic access to NASA servers",
    "text": "Configure programmatic access to NASA servers\nIf you use web interfaces to retrieve nasa data such as Earthdata Search you are prompted to login. We will be using software to retrieve data from NASA Servers during the hackweek, so you must store your credentials on the JupyterHub. Run the following commands on the JupyterHub in a terminal replacing your Earthdata login username and password:\necho \"machine urs.earthdata.nasa.gov login EARTHDATA_LOGIN password EARTHDATA_PASSWORD\" &gt; ~/.netrc\nchmod 0600 .netrc",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Earthdata login"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "href": "content/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "title": "Intro to JupyterHubs",
    "section": "Log into the JupyterHub",
    "text": "Log into the JupyterHub\nGo to https://dhub.opensci.live/. Click “Login to continue”. You will be asked to log in with your GitHub Account, if you are not logged in already.\n\nImage type: Python or R\nNext you select your image type. We will use the default Py-R - base geospatial image.\n\n\nVirtual Machine size\nYou’ll see something similar to this that allows you to choose a large virtual machine if your project needs it. For the tutorials, you will only need the Small Virtual Machine. Please only choose the large machines if you run out of RAM as the larger machines cost us more.\n\n\n\nMachine Profiles\n\n\n\n\nStart up\nAfter we select our server type and click on start, JupyterHub will allocate our instance in the cloud (on Azure). This may take several minutes.\n\n\n\nJupyterhub Spawning",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#open-rstudio",
    "href": "content/01-intro-to-jupyterhub.html#open-rstudio",
    "title": "Intro to JupyterHubs",
    "section": "Open RStudio",
    "text": "Open RStudio\nWhen you are in the Jupyter Lab tab (note the Jupyter Logo), you will see a Launcher page. If you don’t see this, go to File &gt; New Launcher.\n\n\n\nJupyterhub Launcher\n\n\nIf you will be using Python today, you can stay in Jupyter Lab. If you are using R today then read the next steps.\n\nOpen RStudio by clicking on the “RStudio” box in the Launcher tab:\n\n\n\n\nRStudio",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#end-your-session",
    "href": "content/01-intro-to-jupyterhub.html#end-your-session",
    "title": "Intro to JupyterHubs",
    "section": "End your session",
    "text": "End your session\nWhen you are finished working for the day it is important to log out of the Jupyter Hub. When you keep a session active it uses up cloud resources (costs money) and keeps a series of virtual machines deployed.\n\n\n\n\n\n\nCaution\n\n\n\nYou log out from the Jupyter Lab tab not the RStudio tab.\n\n\nFrom the Jupyter Lab tab, do one of two things to stop the server:\n\nLog out File -&gt; Log Out and click “Log Out”!\nor File -&gt; Hub Control Panel -&gt; Stop My Server\n\n\n\n\n\n\n\nTip\n\n\n\nCan’t find the Jupyter Lab tab? Go to https://dhub.opensci.live/hub/home",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#restart-your-server",
    "href": "content/01-intro-to-jupyterhub.html#restart-your-server",
    "title": "Intro to JupyterHubs",
    "section": "Restart your server",
    "text": "Restart your server\nSometimes the server will crash/stop. This can happen if too many people use a lot of memory all at once. If that happens, go to the Jupyter Lab tab and then File -&gt; Hub Control Panel -&gt; Stop My Server and then Start My Server. You shouldn’t lose your work unless you were uploading a file.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#your-files",
    "href": "content/01-intro-to-jupyterhub.html#your-files",
    "title": "Intro to JupyterHubs",
    "section": "Your files",
    "text": "Your files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/jovyan. Everyone has the same home directory but your files are separate and cannot be seen by others.\nThere are a number of different ways to create new files. We will practice this in the RStudio lecture.\n\nWill I lose all of my work?\nLogging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#shared-files",
    "href": "content/01-intro-to-jupyterhub.html#shared-files",
    "title": "Intro to JupyterHubs",
    "section": "Shared files",
    "text": "Shared files\n\n\n\nShared folder\n\n\nIn the file panel, you will see a folder called shared. These are read-only shared files.\nYou will also see shared-public. This is a read-write folder for you to put files for everyone to see and use. You can create a team folder here for shared data and files. Note, everyone can see and change these so be careful to communicate with your team so multiple people don’t work on the same file at the same time. You can also create folders for each team member and agree not to change other team members files.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#python-users",
    "href": "content/01-intro-to-jupyterhub.html#python-users",
    "title": "Intro to JupyterHubs",
    "section": "**Python users",
    "text": "**Python users\nYou can open a Jupyter Notebook by clicking on the “Python 3” box. In the Launcher tab:\n\n\n\nJupyterhub Launcher\n\n\nJupyter notebooks are a very common way to share Python code and tutorials. Get an overview of Jupyter Lab: Intro to Jupyter Lab Learn about the geosciences tools in Python",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "content/01-intro-to-jupyterhub.html#faq",
    "href": "content/01-intro-to-jupyterhub.html#faq",
    "title": "Intro to JupyterHubs",
    "section": "FAQ",
    "text": "FAQ\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‘jupyter’ based images/dockers. It is the historic home directory for Jupyter deployments.\nCan other users see the files in my /home/jovyan folder? No, other users can not see your credentials.\n\nAcknowledgements\nSome sections of this document have been taken from hackweeks organized by the University of Washington eScience Institute and Openscapes.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Jupyter hubs"
    ]
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to providing a harassment-free learning experience for everyone regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age or religion. We do not tolerate harassment of participants in any form. Sexual language and imagery is not appropriate either in-person or virtual form, including the Discussion boards and Slack workspace. Participants (including event volunteers and organizers) violating these rules may be sanctioned or expelled from the event at the discretion of the organizers.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#definition-of-harassment",
    "href": "coc.html#definition-of-harassment",
    "title": "Code of Conduct",
    "section": "Definition of Harassment",
    "text": "Definition of Harassment\nHarassment includes, but is not limited to:\n\nVerbal comments that reinforce social structures of domination related to gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, age, religion.\nSexual images in public spaces\nDeliberate intimidation, stalking, or following\nHarassing photography or recording\nSustained disruption of talks or other events\nInappropriate physical contact\nUnwelcome sexual attention\nAdvocating for, or encouraging, any of the above behavior",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#expectations",
    "href": "coc.html#expectations",
    "title": "Code of Conduct",
    "section": "Expectations",
    "text": "Expectations\nParticipants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the event.\nThe organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. We expect participants to follow these rules at all the event venues and event-related social activities.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#reporting-a-violation",
    "href": "coc.html#reporting-a-violation",
    "title": "Code of Conduct",
    "section": "Reporting a violation",
    "text": "Reporting a violation\nHarassment and other code of conduct violations reduce the value of the event for everyone. If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.\nIf you feel comfortable contacting someone associated with our event, you may speak with one of the event organizers in person or contact an organizer on a private Slack channel.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "content/01-intro-to-cloud.html",
    "href": "content/01-intro-to-cloud.html",
    "title": "Intro the Cloud",
    "section": "",
    "text": "Lecture on NASA earth data in the cloud by Michele Thornton (NASA Openscapes) Video",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Geoscience cloud tools"
    ]
  },
  {
    "objectID": "content/01-welcome.html",
    "href": "content/01-welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Introduction to working with earth data in the cloud and NASA Earth Data\nOrientation on our JupyterHub\nTutorial 1: Searching for resources in NASA Earth Data\nTutorial 2: Points and shapefiles\nTutorial 3: Subsetting your earth data with in a region (shapefile)\nTutorial 4: Getting values at points (along a track or transect)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Welcome",
      "Welcome"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#summary",
    "href": "content/02-git-jupyter.html#summary",
    "title": "Git - Jupyter Lab",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we will provide a brief introduction to:\n\nCommand line (terminal/shell)\nNavigating around folders in Jupyter Lab\nVersion Control (code management using git)\nSetting up Git in Jupyter Lab\nThe Git GUI in Jupyter Lab\nBasic Git commands",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-JupyterLab"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-jupyter-lab",
    "href": "content/02-git-jupyter.html#introduction-jupyter-lab",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Jupyter Lab",
    "text": "Introduction :: Jupyter Lab\nWhen you start the JupyterHub, you will be in Jupyter Lab. From there you can click on the RStudio box and open RStudio. However for this tutorial, we will stay in Juptyer Lab.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-JupyterLab"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-terminalshell",
    "href": "content/02-git-jupyter.html#introduction-terminalshell",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Terminal/Shell",
    "text": "Introduction :: Terminal/Shell\nLog into the JupyterHub. If you do not see this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\nShell or Terminal Basics\n\nWhat is Terminal or Shell?\nNavigating Files and Directories\nWorking with Files and Directories\nOptional: Detailed self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\n\nYou will need only basic navigation skills for this course: cd, ls and cat\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\n\n\n\nLet’s try\nls\nls -a\ncd shared\nls\ncd shell-tutorial\ncat lesson1.sh\ncd ..\ncd ..\n\n\nClose the terminal\nJust click on the X in the terminal tab",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-JupyterLab"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-file-navigation",
    "href": "content/02-git-jupyter.html#introduction-file-navigation",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: File Navigation",
    "text": "Introduction :: File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on shared. Now you can see the files in the shared directory.\nClick on shell-tutorial. Then click on lesson1.sh. The file opens. You won’t be able to save changes here because you don’t have write permission on this drive.\nClick on the folder icon that looks like this. Click on the actual folder image. \nNow it should look like this folder /\nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\nThen click on lesson-scripts to enter the folder\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\nDelete the file by right-clicking on it and clicking “Delete”",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-JupyterLab"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#introduction-version-control-git",
    "href": "content/02-git-jupyter.html#introduction-version-control-git",
    "title": "Git - Jupyter Lab",
    "section": "Introduction :: Version Control (Git)",
    "text": "Introduction :: Version Control (Git)\n\nWhat is version control, git, github, and how to set it up?\nVersion control is managing and tracking changes to your documents (program source code, images, websites, data files, etc.). git is a popular tool used for version control of software code. github.com is popular platform that provides remote server hosting for git repositories. A repository is a collection of various files that you are tracking for changes and versions. Currently GitHub is the most popular platform for file sharing code and code packages.\nThis section is a step-by-step guide to set up git on our JupyterHub. We will also configure git to use your github.com account for managing your repositories hosted on github.com. There are 5 main steps.\n\n\nStep 1: Create a GitHub account\nTo complete the setup, you will need an account on github.com. If you don’t have an account, please visit github.com, create an account (free) and come back to this guide for setting up git.\n\n\nStep 2: Clone a repository\nWe have created a demo repository for you to clone:\nhttps://github.com/nmfs-opensci/Git-Lesson\n\nStart your JupyterHub\nClick on the Git icon\n\n\n\nClick “Clone a Repository”\nWhere it says “Enter the URI of the remote Git repository”, paste in the URL https://github.com/nmfs-opensci/EDMW-EarthData-Workshop-2024\nThe folder appears and you can enter the folder and edit and create files.\n\n\nYour task: Create a file with your name and save to the Git-Lesson folder",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-JupyterLab"
    ]
  },
  {
    "objectID": "content/02-git-jupyter.html#step-3",
    "href": "content/02-git-jupyter.html#step-3",
    "title": "Git - Jupyter Lab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on the JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git Jupyter lab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-JupyterLab"
    ]
  },
  {
    "objectID": "content/02-git.html#set-up-authentication-to-github",
    "href": "content/02-git.html#set-up-authentication-to-github",
    "title": "Git - Set-up for Pushing",
    "section": "Set up authentication to GitHub",
    "text": "Set up authentication to GitHub\nYou need to tell GitHub who you are so you can push your local changes up to GitHub. There are a few ways to do this. I am going to show you a way that works on any computer, including a virtual computer like the JupyterHub.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the “repo” checkbox, and then scroll to bottom to click “Generate”.\nFor scope, select “repo”.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nOpen a terminal. In Jupyter Lab, you will see a box labelled “Terminal” on the Launcher window. In RStudio, you will see a tab (usually in lower left) with the label “Terminal”\nPaste these 3 lines of code into the terminal\n\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false\ngit config --global credential.helper store\nReplace \"&lt;your email&gt;\" with something like jane.doe@noaa.gov. Replace \"&lt;your name&gt;\" with something like \"Jane Doe\". Notice the quotes.\n\n\nStep 3: Trigger git to ask for your password\nThere are a few ways to do this.\n\nClone a repo, make a change, and then commit and push the change\nClone a private repo\n\nOption b is easiest if you are new to Git and GitHub.\n\nOpen a terminal window\nMake sure you are in the home directory by typing cd ~\nClone a repo and create an RStudio project. File &gt; New Project &gt; Version Control &gt; Git. Paste in this URL https://github.com/nmfs-opensci/github_setup_check and make sure it is creating the repo at ~ (home directory).\nYou will be asked for your GitHub username and password. For the password, enter the PERSONAL ACCESS TOKEN from Step 1.\n\nWatch a video of these 4 steps\nFull instructions with other ways to do this from R",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Git-Authentication"
    ]
  },
  {
    "objectID": "content/02-local-setup-r.html",
    "href": "content/02-local-setup-r.html",
    "title": "Setting up on your computer - R users",
    "section": "",
    "text": "Here are instructions for installing on your own computer.\nInstall the development version of earthdatalogin and update terra.\ndevtools::install_github(\"boettiger-lab/earthdatalogin\")\ninstall.packages(\"terra\")\ninstall.packages(\"rstac\")\ninstall.packages(\"gdalcubes\")\ninstall.packages(\"here\")\n\nlibrary(\"earthdatalogin\")\nlibrary(\"terra\")\nlibrary(\"rstac\")\nlibrary(\"gdalcubes\")\nlibrary(\"here\")\nYou will need GDAL installed. See these instructions if you do not have it installed: https://developers.planet.com/docs/integrations/qgis/install-qgis-gdal/\nYou may need to install terra and sf from source to get them to use the latest GDAL installation.\ninstall.packages(\"terra\", type = \"source\")\ninstall.packages(\"sf\", type = \"source\")\nsf::sf_extSoftVersion()\nThe environment we are using today is the py-rocket-geospatial image. This is part of work on a Data Science Docker Stack for NOAA Fisheries.",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Local set-up - R"
    ]
  },
  {
    "objectID": "content/02-local-setup-r.html#load-packages",
    "href": "content/02-local-setup-r.html#load-packages",
    "title": "Setting up on your computer - R users",
    "section": "Load packages",
    "text": "Load packages\n\n# Function to check if pkgs are installed, and install any missing pkgs\npkgTest &lt;- function(x)\n{\n  if (!require(x,character.only = TRUE))\n  {\n    install.packages(x,dep=TRUE,repos='http://cran.us.r-project.org')\n    if(!require(x,character.only = TRUE)) stop(x, \" :Package not found\")\n  }\n}\n\n# Create list of required packages\nlist.of.packages &lt;- c(\n  \"earthdatalogin\", \"terra\", \"rstac\", \"gdalcubes\", \"here\", \"sf\", \"dplyr\", \"stars\", \"tmap\", \n  \"rerddap\", \"plotdap\", \"parsedate\", \"ggplot2\", \"rerddapXtracto\",\n  \"date\", \"maps\", \"mapdata\", \"RColorBrewer\",\"viridis\")\n\n# Create list of installed packages\npkges = installed.packages()[,\"Package\"]\n\n# Install and load all required pkgs\nfor (pk in list.of.packages) {\n  pkgTest(pk)\n}\n\nLoading required package: earthdatalogin\n\n\nLoading required package: terra\n\n\nterra 1.7.71\n\n\nLoading required package: rstac\n\n\nLoading required package: gdalcubes\n\n\n\nAttaching package: 'gdalcubes'\n\n\nThe following objects are masked from 'package:terra':\n\n    animate, crop, size\n\n\nLoading required package: here\n\n\nhere() starts at /home/jovyan/EDMW-EarthData-Workshop-2024\n\n\nLoading required package: sf\n\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:terra':\n\n    intersect, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: stars\n\n\nLoading required package: abind\n\n\nLoading required package: tmap\n\n\n\nAttaching package: 'tmap'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n\nLoading required package: rerddap\n\n\nRegistered S3 method overwritten by 'hoardr':\n  method           from\n  print.cache_info httr\n\n\nLoading required package: plotdap\n\n\nLoading required package: parsedate\n\n\nLoading required package: ggplot2\n\n\nLoading required package: rerddapXtracto\n\n\nLoading required package: date\n\n\nLoading required package: maps\n\n\nLoading required package: mapdata\n\n\nLoading required package: RColorBrewer\n\n\nLoading required package: viridis\n\n\nLoading required package: viridisLite\n\n\n\nAttaching package: 'viridis'\n\n\nThe following object is masked from 'package:maps':\n\n    unemp\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: Etc/UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] viridis_0.6.5           viridisLite_0.4.2       RColorBrewer_1.1-3     \n [4] mapdata_2.3.1           maps_3.4.2              date_1.2-42            \n [7] rerddapXtracto_1.2.0    ggplot2_3.5.1           parsedate_1.3.1        \n[10] plotdap_1.0.3           rerddap_1.1.0           tmap_3.99.9001         \n[13] stars_0.6-5             abind_1.4-5             dplyr_1.1.4            \n[16] sf_1.0-16               here_1.0.1              gdalcubes_0.7.0        \n[19] rstac_1.0.0             terra_1.7-71            earthdatalogin_0.0.2.99\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        fastmap_1.1.1           lazyeval_0.2.2         \n [4] leaflegend_1.2.0        leaflet_2.2.2           XML_3.99-0.16.1        \n [7] digest_0.6.35           lifecycle_1.0.4         magrittr_2.0.3         \n[10] compiler_4.4.0          rlang_1.1.3             tools_4.4.0            \n[13] utf8_1.2.4              yaml_2.3.8              data.table_1.15.4      \n[16] knitr_1.46              htmlwidgets_1.6.4       sp_2.1-4               \n[19] classInt_0.4-10         curl_5.2.1              xml2_1.3.6             \n[22] KernSmooth_2.23-22      httpcode_0.3.0          withr_3.0.0            \n[25] purrr_1.0.2             leafsync_0.1.0          grid_4.4.0             \n[28] fansi_1.0.6             cols4all_0.7-1          e1071_1.7-14           \n[31] leafem_0.2.3            colorspace_2.1-0        spacesXYZ_1.3-0        \n[34] scales_1.3.0            dichromat_2.0-0.1       crul_1.4.2             \n[37] cli_3.6.2               rmarkdown_2.26          crayon_1.5.2           \n[40] generics_0.1.3          rstudioapi_0.16.0       httr_1.4.7             \n[43] tmaptools_3.1-1         ncdf4_1.22              DBI_1.2.2              \n[46] proxy_0.4-27            parallel_4.4.0          base64enc_0.1-3        \n[49] vctrs_0.6.5             jsonlite_1.8.8          jpeg_0.1-10            \n[52] crosstalk_1.2.1         tidyr_1.3.1             units_0.8-5            \n[55] glue_1.7.0              lwgeom_0.2-14           codetools_0.2-20       \n[58] leaflet.providers_2.0.0 gtable_0.3.5            raster_3.6-26          \n[61] munsell_0.5.1           tibble_3.2.1            pillar_1.9.0           \n[64] rappdirs_0.3.3          htmltools_0.5.8.1       R6_2.5.1               \n[67] hoardr_0.5.4            rprojroot_2.0.4         evaluate_0.23          \n[70] lattice_0.22-6          png_0.1-8               class_7.3-22           \n[73] Rcpp_1.0.12             gridExtra_2.3           widgetframe_0.3.1      \n[76] xfun_0.43               pkgconfig_2.0.3",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Orientation",
      "Local set-up - R"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Week 1 Tutorials",
    "section": "",
    "text": "During week 1, participants will gain experience with the platforms used in collaborative science: GitHub and RMarkdown."
  },
  {
    "objectID": "content/index.html#prerequisites",
    "href": "content/index.html#prerequisites",
    "title": "Week 1 Tutorials",
    "section": "Prerequisites",
    "text": "Prerequisites\nPlease follow the set up prerequisites"
  },
  {
    "objectID": "content/index.html#content",
    "href": "content/index.html#content",
    "title": "Week 1 Tutorials",
    "section": "Content",
    "text": "Content\n\nThe R language and RStudio\nIntro to RStudio\nIntroduction to Git and GitHub"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "All times Eastern. May 15, 2024",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#topics",
    "href": "schedule.html#topics",
    "title": "Schedule",
    "section": "Topics",
    "text": "Topics\n\nTutorial 1: Searching for resources in NASA Earthdata\nTutorial 2: Plotting and subsetting earth data\nTutorial 3: Masking and aggregating your earth data within a region (shapefiles)",
    "crumbs": [
      "JupyterHub",
      "Welcome",
      "Schedule"
    ]
  },
  {
    "objectID": "support.html#thank-you-for-inspiration-and-content",
    "href": "support.html#thank-you-for-inspiration-and-content",
    "title": "Acknowledgements",
    "section": "Thank you for inspiration and content!",
    "text": "Thank you for inspiration and content!\nThank you to the open science community that has created software, teaching resources, and workflows that we have been able to build off of and be inspired by. These include: NASA Openscapes • OceanHackWeek • SnowEx Hackweek • eScience Institute, University of Washington • ICESat-2 Hackweek • Project Jupyter • Pangeo Project • CryoCloud"
  },
  {
    "objectID": "tutorials/python/1-earthaccess.html#summary",
    "href": "tutorials/python/1-earthaccess.html#summary",
    "title": "Earthdata Search and Discovery",
    "section": "Summary",
    "text": "Summary\nIn this example we will use the earthaccess library to search for data collections from NASA Earthdata. earthaccess is a Python library that simplifies data discovery and access to NASA Earth science data by providing an abstraction layer for NASA’s Common Metadata Repository (CMR) API Search API. The library makes searching for data more approachable by using a simpler notation instead of low level HTTP queries. earthaccess takes the trouble out of Earthdata Login authentication, makes search easier, and provides a stream-line way to download or stream search results into an xarray object.\nFor more on earthaccess visit the earthaccess GitHub page and/or the earthaccess documentation site. Be aware that earthaccess is under active development.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/python/1-earthaccess.html#prerequisites",
    "href": "tutorials/python/1-earthaccess.html#prerequisites",
    "title": "Earthdata Search and Discovery",
    "section": "Prerequisites",
    "text": "Prerequisites\nAn Earthdata Login account is required to access data from NASA Earthdata. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/python/1-earthaccess.html#get-started",
    "href": "tutorials/python/1-earthaccess.html#get-started",
    "title": "Earthdata Search and Discovery",
    "section": "Get Started",
    "text": "Get Started\n\nImport Required Packages\n\nimport earthaccess \nfrom pprint import pprint\nimport xarray as xr\nimport geopandas as gpd\n\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)\n\n\n\nSearch for data\nThere are multiple keywords we can use to discovery data from collections. The table below contains the short_name, concept_id, and doi for some collections we are interested in for other exercises. Each of these can be used to search for data or information related to the collection we are interested in.\n\n\n\n\n\n\n\n\nShortname\nCollection Concept ID\nDOI\n\n\n\n\nMUR-JPL-L4-GLOB-v4.1\nC1996881146-POCLOUD\n10.5067/GHGMR-4FJ04\n\n\nAVHRR_OI-NCEI-L4-GLOB-v2.1\nC2036881712-POCLOUD\n10.5067/GHAAO-4BC21\n\n\n\nHow can we find the shortname, concept_id, and doi for collections not in the table above?. Let’s take a quick detour.\nhttps://search.earthdata.nasa.gov/search\n\nSearch by collection\nWe will use the GHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis data from NCEI which has collection id C2036881712-POCLOUD.\n\ncollection_id = 'C2036881712-POCLOUD'\nresults = earthaccess.search_data(\n    concept_id = collection_id\n)\n\nGranules found: 3052\n\n\nIn this example we used the concept_id parameter to search from our desired collection. However, there are multiple ways to specify the collection(s) we are interested in. Alternative parameters include:\n\ndoi - request collection by digital object identifier (e.g., doi = ‘10.5067/GHAAO-4BC21’)\n\nshort_name - request collection by CMR shortname (e.g., short_name = ‘AVHRR_OI-NCEI-L4-GLOB-v2.1’)\n\nNOTE: Each Earthdata collection has a unique concept_id and doi. This is not the case with short_name. A shortname can be associated with multiple versions of a collection. If multiple versions of a collection are publicaly available, using the short_name parameter with return all versions available. It is advised to use the version parameter in conjunction with the short_name parameter with searching.\nWe can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box.\nFor our bounding box, we need the xmin, ymin, xmax, ymax and we will assign this to bbox. We will assign our start date and end date to a variable named date_range . We can also specify that we only want cloud hosted data.\n\ndate_range = (\"2020-01-16\", \"2020-12-16\")\n# (xmin=-73.5, ymin=33.5, xmax=-43.5, ymax=43.5)\nbbox = (-73.5, 33.5, -43.5, 43.5)\n\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n    temporal = date_range,\n    bounding_box = bbox,\n)\n\nGranules found: 337\n\n\n\nThe short_name and concept_id search parameters can be used to request one or multiple collections per request, but the doi parameter can only request a single collection.\n&gt; concept_ids = [‘C2723754864-GES_DISC’, ‘C1646609808-NSIDC_ECS’]\n\nUse the cloud_hosted search parameter only to search for data assets available from NASA’s Earthdata Cloud.\nThere are even more search parameters that can be passed to help refine our search, however those parameters do have to be populated in the CMR record to be leveraged. A non exhaustive list of examples are below:\n\nday_night_flag = 'day'\n\ncloud_cover = (0, 10)\n\n\n\n\n\nWorking with earthaccess returns\nFollowing the search for data, you’ll likely take one of two pathways with those results. You may choose to download the assets that have been returned to you or you may choose to continue working with the search results within the Python environment.\n\ntype(results[0])\n\nearthaccess.results.DataGranule\n\n\n\nresults[0]\n\n\n    \n            \n            \n    \n      \n        \n          \n            Data: 20200115120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc\n            Size: 0.99 MB\n            Cloud Hosted: True\n          \n          \n            \n          \n        \n      \n    \n    \n\n\n\nDownload earthaccess results\nIn some cases you may want to download your assets. earthaccess makes downloading the data from the search results very easy using the earthaccess.download() function.\ndownloaded_files = earthaccess.download( results[0:9], local_path=‘../data’, )\nearthaccess does a lot of heavy lifting for us. It identifies the downloadable links, passes our Earthdata Login credentials, and saves the files with the proper names.\n\n\nWork in the cloud\nAlternatively we can work with the metadata without downloading and only load the data into memory (or download) when we need to compute with it or plot it.\nThe data_links() methods gets us the url to the data. The data_links() method can also be used to get the s3 URI when we want to perform direct s3 access of the data in the cloud. To get the s3 URI, pass access = 'direct' to the method. Note, for NASA data, you need to be in AWS us-west-2 for direct access to work.\n\nresults[0].data_links()\n\n['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20200115120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc']\n\n\nWe can pass or read the data url into libraries like xarray, rioxarray, or gdal, but we would need to deal with the authentication. earthaccess has a built-in module for easily reading these data links in and takes care of authentication for us.\nWe use earthaccess’s open() method make a connection the cloud resource so we can work with the files. To get the first file, we use results[0:1].\n\nfileset = earthaccess.open(results[0:1])\n\nOpening 1 granules, approx size: 0.0 GB\n\n\n\n\n\n\n\n\n\n\n\n\nds = xr.open_dataset(fileset[0])\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 17MB\nDimensions:           (lat: 720, lon: 1440, time: 1, nv: 2)\nCoordinates:\n  * lat               (lat) float32 3kB -89.88 -89.62 -89.38 ... 89.62 89.88\n  * lon               (lon) float32 6kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time              (time) datetime64[ns] 8B 2020-01-15\nDimensions without coordinates: nv\nData variables:\n    lat_bnds          (lat, nv) float32 6kB ...\n    lon_bnds          (lon, nv) float32 12kB ...\n    analysed_sst      (time, lat, lon) float32 4MB ...\n    analysis_error    (time, lat, lon) float32 4MB ...\n    mask              (time, lat, lon) float32 4MB ...\n    sea_ice_fraction  (time, lat, lon) float32 4MB ...\nAttributes: (12/47)\n    Conventions:                CF-1.6, ACDD-1.3\n    title:                      NOAA/NCEI 1/4 Degree Daily Optimum Interpolat...\n    id:                         NCEI-L4LRblend-GLOB-AVHRR_OI\n    references:                 Reynolds, et al.(2009) What is New in Version...\n    institution:                NOAA/NESDIS/NCEI\n    creator_name:               NCEI Products and Services\n    ...                         ...\n    Metadata_Link.:             http://doi.org/10.7289/V5SQ8XB5\n    keywords:                   Oceans&gt;Ocean Temperature&gt;Sea Surface Temperature\n    keywords_vocabulary:        NASA Global Change Master Directory (GCMD) Sc...\n    standard_name_vocabulary:   CF Standard Name Table v29\n    processing_level:           L4\n    cdm_data_type:              Gridxarray.DatasetDimensions:lat: 720lon: 1440time: 1nv: 2Coordinates: (3)lat(lat)float32-89.88 -89.62 ... 89.62 89.88long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([-89.875, -89.625, -89.375, ...,  89.375,  89.625,  89.875],\n      dtype=float32)lon(lon)float32-179.9 -179.6 ... 179.6 179.9long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-179.875, -179.625, -179.375, ...,  179.375,  179.625,  179.875],\n      dtype=float32)time(time)datetime64[ns]2020-01-15long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-15T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (6)lat_bnds(lat, nv)float32...units :degrees_northcomment :This variable defines the latitude values at the north and south bounds of every 0.25-degree pixel.[1440 values with dtype=float32]lon_bnds(lon, nv)float32...units :degrees_eastcomment :This variable defines the longitude values at the west and east bounds of every 0.25-degree pixel.[2880 values with dtype=float32]analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[1036800 values with dtype=float32]analysis_error(time, lat, lon)float32...long_name :estimated error standard deviation of analysed_sstunits :kelvinvalid_min :0valid_max :32767comment :Sum of bias, sampling and random errors.[1036800 values with dtype=float32]mask(time, lat, lon)float32...long_name :sea/land field composite maskvalid_min :1valid_max :31flag_masks :[ 1  2  4  8 16]flag_meanings :water land optional_lake_surface sea_ice optional_river_surfacesource :RWReynolds_landmask_V1.0comment :Several masks distinguishing between water, land and ice.[1036800 values with dtype=float32]sea_ice_fraction(time, lat, lon)float32...long_name :sea ice area fractionstandard_name :sea_ice_area_fractionunits :1valid_min :0valid_max :100source :MMAB_50KM-NCEP-ICEcomment :7-day median filtered. Switch from 25 km NASA team ice (http://nsidc.org/data/nsidc-0051.html) to 50 km NCEP ice (http://polar.ncep.noaa.gov/seaice) after 2004 results in artificial increase in ice coverage.[1036800 values with dtype=float32]Indexes: (3)latPandasIndexPandasIndex(Index([-89.875, -89.625, -89.375, -89.125, -88.875, -88.625, -88.375, -88.125,\n       -87.875, -87.625,\n       ...\n        87.625,  87.875,  88.125,  88.375,  88.625,  88.875,  89.125,  89.375,\n        89.625,  89.875],\n      dtype='float32', name='lat', length=720))lonPandasIndexPandasIndex(Index([-179.875, -179.625, -179.375, -179.125, -178.875, -178.625, -178.375,\n       -178.125, -177.875, -177.625,\n       ...\n        177.625,  177.875,  178.125,  178.375,  178.625,  178.875,  179.125,\n        179.375,  179.625,  179.875],\n      dtype='float32', name='lon', length=1440))timePandasIndexPandasIndex(DatetimeIndex(['2020-01-15'], dtype='datetime64[ns]', name='time', freq=None))Attributes: (47)Conventions :CF-1.6, ACDD-1.3title :NOAA/NCEI 1/4 Degree Daily Optimum Interpolation Sea Surface Temperature (OISST) Analysis, Version 2 - Finalid :NCEI-L4LRblend-GLOB-AVHRR_OIreferences :Reynolds, et al.(2009) What is New in Version 2. Available at http://www.ncdc.noaa.gov/sites/default/files/attachments/Reynolds2009_oisst_daily_v02r00_version2-features.pdf;Daily 1/4 Degree Optimum Interpolation Sea Surface Temperature (OISST) - Climate Algorithm Theoretical Basis Document, NOAA Climate Data Record Program CDRP-ATBD-0303 Rev. 2 (2013). Available at http://www1.ncdc.noaa.gov/pub/data/sds/cdr/CDRs/Sea_Surface_Temperature_Optimum_Interpolation/AlgorithmDescription.pdf.institution :NOAA/NESDIS/NCEIcreator_name :NCEI Products and Servicescreator_email :ncei.orders@noaa.govcreator_url :http://www.ncdc.noaa.gov/oisstgds_version_id :v2.0r5netcdf_version_id :4.3.2date_created :20200211T000000Zproduct_version :Version 2.0history :2015-10-28: Modified format and attributes with NCO to match the GDS 2.0 rev 5 specification.spatial_resolution :0.25 degreestart_time :20200115T000000Zstop_time :20200116T000000Zwesternmost_longitude :-179.875easternmost_longitude :179.875southernmost_latitude :-89.875northernmost_latitude :89.875file_quality_level :3source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :The daily OISST version 2.0 data contained in this file are the same as those in the equivalent GDS 1.0 file.summary :NOAAs 1/4-degree Daily Optimum Interpolation Sea Surface Temperature (OISST) (sometimes referred to as Reynolds SST, which however also refers to earlier products at different resolution), currently available as version 2, is created by interpolating and extrapolating SST observations from different sources, resulting in a smoothed complete field. The sources of data are satellite (AVHRR) and in situ platforms (i.e., ships and buoys), and the specific datasets employed may change over time. At the marginal ice zone, sea ice concentrations are used to generate proxy SSTs.  A preliminary version of this file is produced in near-real time (1-day latency), and then replaced with a final version after 2 weeks. Note that this is the AVHRR-ONLY DOISST, available from Oct 1981, but there is a companion DOISST product that includes microwave satellite data, available from June 2002.acknowledgement :This project was supported in part by a grant from the NOAA Climate Data Record (CDR) Program. Cite this dataset when used as a source. The recommended citation and DOI depends on the data center from which the files were acquired. For data accessed from NOAA in near real-time or from the GHRSST LTSRF, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. NOAA National Centers for Environmental Information. http://doi.org/doi:10.7289/V5SQ8XB5 [access date]. For data accessed from the NASA PO.DAAC, cite as: Richard W. Reynolds, Viva F. Banzon, and NOAA CDR Program (2008): NOAA Optimum Interpolation 1/4 Degree Daily Sea Surface Temperature (OISST) Analysis, Version 2. [indicate subset used]. PO.DAAC, CA, USA. http://doi.org/10.5067/GHAAO-4BC01 [access date].license :No constraints on data access or use.project :Group for High Resolution Sea Surface Temperaturepublisher_name :NCEI Products and Servicespublisher_email :ncei.orders@noaa.govpublisher_url :http://www.ncdc.noaa.gov/oisstnaming_authority :org.ghrssttime_coverage_start :20200115T000000Ztime_coverage_end :20200116T000000Zplatform :sensor :uuid :15459239-4bd8-4e2c-801a-9c515da7af42geospatial_lat_units :degrees_northgeospatial_lat_resolution :0.25geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.25Metadata_Conventions :ACDD-1.3Metadata_Link. :http://doi.org/10.7289/V5SQ8XB5keywords :Oceans&gt;Ocean Temperature&gt;Sea Surface Temperaturekeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywords, Version 8.1standard_name_vocabulary :CF Standard Name Table v29processing_level :L4cdm_data_type :Grid\n\n\nWe can plot this object.\n\nds['analysed_sst'].plot()",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/python/1-earthaccess.html#conclusion",
    "href": "tutorials/python/1-earthaccess.html#conclusion",
    "title": "Earthdata Search and Discovery",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes tutorial 1. You have worked with remote-sensing data in the cloud and plotted a single file.\nNext we will learn to subset the data so we can work with bigger datasets in the cloud without downloading the whole dataset.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/python/1-earthaccess.html#resources",
    "href": "tutorials/python/1-earthaccess.html#resources",
    "title": "Earthdata Search and Discovery",
    "section": "Resources",
    "text": "Resources\n\nNASA’s Common Metadata Repository (CMR) API\n\nearthaccess repository\nearthaccess documentation\nEarthdata Search",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#summary",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#summary",
    "title": "Extract data within a boundary",
    "section": "Summary",
    "text": "Summary\nIn this example, we will utilize the earthdatalogin R package to retrieve sea surface temperature data from NASA Earthdata.\nThis example is adapted from the NOAA CoastWatch Satellite Data Tutorials. To explore the full range of tutorials on accessing and utilizing oceanographic satellite data, visit the NOAA CoastWatch Tutorial Github repository.\nFor more on earthdatalogin visit the earthdatalogin GitHub page and/or the earthdatalogin documentation site. Be aware that earthdatalogin is under active development.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#prerequisites",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#prerequisites",
    "title": "Extract data within a boundary",
    "section": "Prerequisites",
    "text": "Prerequisites",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#datasets-used",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#datasets-used",
    "title": "Extract data within a boundary",
    "section": "Datasets used",
    "text": "Datasets used\nGHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis (GDS2) from NCEI\nThis NOAA blended SST is a moderate resolution satellite-based gap-free sea surface temperature (SST) product. We will use the daily data. https://cmr.earthdata.nasa.gov/search/concepts/C2036881712-POCLOUD.html\nLonghurst Marine Provinces\nThe dataset represents the division of the world oceans into provinces as defined by Longhurst (1995; 1998; 2006). This division has been based on the prevailing role of physical forcing as a regulator of phytoplankton distribution.\nThe Longhurst Marine Provinces dataset is available online (https://www.marineregions.org/downloads.php) and within the shapes folder associated with this repository. For this exercise we will use the Gulf Stream province (ProvCode: GFST)\n\n\n\n../images/longhurst.png",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#datasets-used-1",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#datasets-used-1",
    "title": "Extract data within a boundary",
    "section": "Datasets used",
    "text": "Datasets used\nBlended Analysis Sea-Surface Temperature Daily Data\nWe will use the GHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis data from NCEI which has collection id C2036881712-POCLOUD.\nLonghurst Marine Provinces\nThe dataset represents the division of the world oceans into provinces as defined by Longhurst (1995; 1998; 2006). This division has been based on the prevailing role of physical forcing as a regulator of phytoplankton distribution. The Longhurst Marine Provinces dataset is available online (https://www.marineregions.org/downloads.php) and within the shapes folder associated with this repository. For this tutorial we will use the Gulf Stream province (ProvCode: GFST)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#import-packages",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#import-packages",
    "title": "Extract data within a boundary",
    "section": "Import packages",
    "text": "Import packages\n\nimport earthaccess \nfrom pprint import pprint\nimport xarray as xr\nimport geopandas as gpd\n\nimport regionmask\nimport os\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nos.environ[\"HOME\"] = \"/home/jovyan\" \n\n\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#load-the-longhurst-provinces-shape-files-into-a-geopandas-dataframe",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#load-the-longhurst-provinces-shape-files-into-a-geopandas-dataframe",
    "title": "Extract data within a boundary",
    "section": "Load the Longhurst Provinces shape files into a geopandas dataframe",
    "text": "Load the Longhurst Provinces shape files into a geopandas dataframe\n\n#shape_path = '../resources/longhurst_v4_2010/Longhurst_world_v4_2010.shp'\nshape_path = os.path.join('..',\n                          'resources',\n                          'longhurst_v4_2010',\n                          'Longhurst_world_v4_2010.shp'\n                          )\nshapefiles = gpd.read_file(shape_path)\nshapefiles.head(8)\n\n\n\n\n\n\n\n\n\nProvCode\nProvDescr\ngeometry\n\n\n\n\n0\nBPLR\nPolar - Boreal Polar Province (POLR)\nMULTIPOLYGON (((-161.18426 63.50000, -161.5000...\n\n\n1\nARCT\nPolar - Atlantic Arctic Province\nMULTIPOLYGON (((-21.51305 64.64409, -21.55945 ...\n\n\n2\nSARC\nPolar - Atlantic Subarctic Province\nMULTIPOLYGON (((11.26472 63.96082, 11.09548 63...\n\n\n3\nNADR\nWesterlies - N. Atlantic Drift Province (WWDR)\nPOLYGON ((-11.50000 57.50000, -11.50000 56.500...\n\n\n4\nGFST\nWesterlies - Gulf Stream Province\nPOLYGON ((-43.50000 43.50000, -43.50000 42.500...\n\n\n5\nNASW\nWesterlies - N. Atlantic Subtropical Gyral Pro...\nPOLYGON ((-39.50000 25.50000, -40.50000 25.500...\n\n\n6\nNATR\nTrades - N. Atlantic Tropical Gyral Province (...\nMULTIPOLYGON (((-72.34673 18.53597, -72.36877 ...\n\n\n7\nWTRA\nTrades - Western Tropical Atlantic Province\nPOLYGON ((-19.50000 -6.50000, -20.50000 -6.500...",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#isolate-the-gulf-stream-province",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#isolate-the-gulf-stream-province",
    "title": "Extract data within a boundary",
    "section": "Isolate the Gulf Stream Province",
    "text": "Isolate the Gulf Stream Province\nThe Gulf Stream Province can be isolated using its ProvCode (GFST)\n\nProvCode = \"GFST\"\n\n# Locate the row with the ProvCode code\ngulf_stream = shapefiles.loc[shapefiles[\"ProvCode\"] == ProvCode]\ngulf_stream\n\n\n\n\n\n\n\n\n\nProvCode\nProvDescr\ngeometry\n\n\n\n\n4\nGFST\nWesterlies - Gulf Stream Province\nPOLYGON ((-43.50000 43.50000, -43.50000 42.500...",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#find-the-coordinates-of-the-bounding-box",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#find-the-coordinates-of-the-bounding-box",
    "title": "Extract data within a boundary",
    "section": "Find the coordinates of the bounding box",
    "text": "Find the coordinates of the bounding box\n\nThe bounding box is the smallest rectangle that will completely enclose the province.\nWe will use the bounding box coordinates to subset the satellite data\n\n\ngs_bnds = gulf_stream.bounds\ngs_bnds\n\n\n\n\n\n\n\n\n\nminx\nminy\nmaxx\nmaxy\n\n\n\n\n4\n-73.5\n33.5\n-43.5\n43.5",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#search-and-access-nasa-earthdata-with-the-collection-concept-id",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#search-and-access-nasa-earthdata-with-the-collection-concept-id",
    "title": "Extract data within a boundary",
    "section": "Search and access NASA Earthdata with the Collection Concept ID",
    "text": "Search and access NASA Earthdata with the Collection Concept ID\n\n\n\n\n\n\n\n\nShortname\nCollection Concept ID\nDOI\n\n\n\n\nAVHRR_OI-NCEI-L4-GLOB-v2.1\nC2036881712-POCLOUD\n10.5067/GHAAO-4BC21\n\n\n\n\n# Search Dataset Unique ID\ncollection_id = 'C2036881712-POCLOUD'\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n)\n\nGranules found: 3053\n\n\n\n# Define date range and bounding box and search\n\ndate_range = (\"2020-01-16\", \"2020-3-16\")\n# (xmin=-73.5, ymin=33.5, xmax=-43.5, ymax=43.5)\nbbox = (gs_bnds.minx, gs_bnds.miny, gs_bnds.maxx, gs_bnds.maxy)\n\n\n# Get results based on date range and bbox\n\nresults = earthaccess.search_data(\n    concept_id = collection_id,\n    cloud_hosted = True,\n    temporal = date_range,\n    bounding_box = bbox,\n)\n\nGranules found: 62\n\n\n\n# Get the first 30 files\nfileset = earthaccess.open(results[1:30])\n\nOpening 29 granules, approx size: 0.03 GB",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#load-the-satellite-data",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#load-the-satellite-data",
    "title": "Extract data within a boundary",
    "section": "Load the satellite data",
    "text": "Load the satellite data\n\n# Load data \nds = xr.open_mfdataset(fileset, chunks = {})\n\n\n# Get SST data \nds_subset = ds['analysed_sst']",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#visualize-the-unmasked-data-on-a-map",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#visualize-the-unmasked-data-on-a-map",
    "title": "Extract data within a boundary",
    "section": "Visualize the unmasked data on a map",
    "text": "Visualize the unmasked data on a map\nThe map shows the full extent of the bounding box\n\nplt.figure(figsize=(14, 10))\n\n# Label axes of a Plate Carree projection with a central longitude of 180:\nax1 = plt.subplot(211, projection=ccrs.PlateCarree(central_longitude=180))\n\n# Use the lon and lat ranges to set the extent of the map\n# the 120, 260 lon range will show the whole Pacific\n# the 15, 55 lat range with capture the range of the data\nax1.set_extent([260, 350, 15, 55], ccrs.PlateCarree())\n\n# set the tick marks to be slightly inside the map extents\n\n# add feature to the map\nax1.add_feature(cfeature.LAND, facecolor='0.6')\nax1.coastlines()\n\n# format the lat and lon axis labels\nlon_formatter = LongitudeFormatter(zero_direction_label=True)\nlat_formatter = LatitudeFormatter()\nax1.xaxis.set_major_formatter(lon_formatter)\nax1.yaxis.set_major_formatter(lat_formatter)\n\nds_subset[0].plot.pcolormesh(ax=ax1, transform=ccrs.PlateCarree(), cmap='jet')\n\nplt.title('Satellite Data Before Masking')\n\nText(0.5, 1.0, 'Satellite Data Before Masking')",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#create-the-region-from-the-shape-file",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#create-the-region-from-the-shape-file",
    "title": "Extract data within a boundary",
    "section": "Create the region from the shape file",
    "text": "Create the region from the shape file\nThe plot shows the shape of the region and its placement along the US East Coast.\n\nregion = regionmask.from_geopandas(gulf_stream)\nregion.plot()",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#mask-the-satellite-data",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#mask-the-satellite-data",
    "title": "Extract data within a boundary",
    "section": "Mask the satellite data",
    "text": "Mask the satellite data\n\n# Create the mask\nmask = region.mask(ds_subset.lon, ds_subset.lat)\n\n# Apply mask the the satellite data\nmasked_ds = ds_subset.where(mask == region.numbers[0])",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#visualize-the-masked-data-on-a-map",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#visualize-the-masked-data-on-a-map",
    "title": "Extract data within a boundary",
    "section": "Visualize the masked data on a map",
    "text": "Visualize the masked data on a map\nThese data have been trimmed to contain only values within the Gulf Stream Province\n\nplt.figure(figsize=(14, 10))\n\n# Label axes of a Plate Carree projection with a central longitude of 180:\nax1 = plt.subplot(211, projection=ccrs.PlateCarree(central_longitude=180))\n\n# Use the lon and lat ranges to set the extent of the map\n# the 120, 260 lon range will show the whole Pacific\n# the 15, 55 lat range with capture the range of the data\nax1.set_extent([260, 350, 15, 55], ccrs.PlateCarree())\n\n# add feature to the map\nax1.add_feature(cfeature.LAND, facecolor='0.6')\nax1.coastlines()\n\nmasked_ds[0].plot.pcolormesh(ax=ax1,\n                             transform=ccrs.PlateCarree(),\n                             cmap='jet')\n\nplt.title('Satellite Data After Masking for Longhurst GFST')\n\nText(0.5, 1.0, 'Satellite Data After Masking for Longhurst GFST')",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#plot-the-mean-daily-temperature-for-the-province",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#plot-the-mean-daily-temperature-for-the-province",
    "title": "Extract data within a boundary",
    "section": "Plot the mean daily temperature for the province",
    "text": "Plot the mean daily temperature for the province\n\ngulf_stream_mean = masked_ds.mean(dim=['lat', 'lon'])\n\n\ngulf_stream_mean\n\nplt.figure(figsize=(10, 5)) \n# Plot the SeaWiFS data\nplt.plot_date(gulf_stream_mean.time,\n              gulf_stream_mean, \n              'o', markersize=8, \n              label='gulf stream', c='black', \n              linestyle='-', linewidth=2) \n\nplt.title('Gulf Stream Province Daily Mean Temperature in 2020')\nplt.ylabel('SST(degrees C)') \nplt.legend()",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/python/3-extract-satellite-data-within-boundary.html#references",
    "href": "tutorials/python/3-extract-satellite-data-within-boundary.html#references",
    "title": "Extract data within a boundary",
    "section": "References",
    "text": "References\nThe several CoastWatch Node websites have data catalog containing documentation and links to all the datasets available:\n* https://oceanwatch.pifsc.noaa.gov/doc.html\n* https://coastwatch.pfeg.noaa.gov/data.html\n* https://polarwatch.noaa.gov/catalog/\nSources for marine shape files * https://www.marineregions.org/downloads.php",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in Python",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#summary",
    "href": "tutorials/r/1-earthdatalogin.html#summary",
    "title": "Earthdata Search and Discovery",
    "section": "Summary",
    "text": "Summary\nIn this example we will use the earthdatalogin R package to search for data collections from NASA Earthdata. earthdatalogin is a R package that simplifies data discovery and access to NASA’s Common Metadata Repository (CMR) API Search API for NASA Earthdata. Despite the name, the NASA Earthdata also holds NOAA data (which we will use today).\nFor more on earthdatalogin visit the earthdatalogin GitHub page and/or the earthdatalogin documentation site. Be aware that earthdatalogin is under active development.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#terminology",
    "href": "tutorials/r/1-earthdatalogin.html#terminology",
    "title": "Earthdata Search and Discovery",
    "section": "Terminology",
    "text": "Terminology\n\nNetCDF files: network Common Data Form; is a file format for storing multidimensional scientific data (variables) such as temperature, humidity, pressure, wind speed, and direction. Each of these variables can be displayed through a dimension (such as time) in ArcGIS by making a layer or table view from the netCDF file. Learn more here.\ntif or tiff or geo tiff file: is used as an interchange format for georeferenced raster imagery. GeoTIFF is in wide use in NASA Earth science data systems. Learn more here.\nraster: is a matrix of cells (or pixels) organized into rows and columns (or a grid) where each cell contains a value representing information, such as temperature. Rasters are digital aerial photographs, imagery from satellites, digital pictures, or even scanned maps.Learn more here.\nGDAL: is a translator library for raster and vector geospatial data formats. As a library, it presents a single raster abstract data model and single vector abstract data model to the calling application for all supported formats. It also comes with a variety of useful command line utilities for data translation and processing. Learn more here.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#prerequisites",
    "href": "tutorials/r/1-earthdatalogin.html#prerequisites",
    "title": "Earthdata Search and Discovery",
    "section": "Prerequisites",
    "text": "Prerequisites\nAn Earthdata Login account is required to access data from NASA Earthdata. Please visit https://urs.earthdata.nasa.gov to register as a new user and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#load-required-packages",
    "href": "tutorials/r/1-earthdatalogin.html#load-required-packages",
    "title": "Earthdata Search and Discovery",
    "section": "Load Required Packages",
    "text": "Load Required Packages\nWe are using the JupyterHub and all necessary packages are already installed for you.\nNote: See the set-up tab (in left nav bar) for instructions on getting set up on your own computer, but be aware that it is common to run into trouble getting GDAL set up properly to handle netCDF files. Using a Docker image (and Python) is often less aggravating.\n\nlibrary(earthdatalogin)\nlibrary(terra)\n\nterra 1.7.71",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#authentication-for-nasa-earthdata",
    "href": "tutorials/r/1-earthdatalogin.html#authentication-for-nasa-earthdata",
    "title": "Earthdata Search and Discovery",
    "section": "Authentication for NASA Earthdata",
    "text": "Authentication for NASA Earthdata\nWe will start by authenticating using our Earthdata Login credentials. Authentication is not necessarily needed to search for publicly available data collections in Earthdata, but is always needed to download or access data from the NASA Earthdata archives. We can use edl_netrc() from the earthdatalogin package to create a .netrc file that will store our credentials.\nThe first time you run authentication use:\n\nearthdatalogin::edl_netrc(\n  username = \"user\", # add your user name\n  password = \"password\" # add you password\n)\n\nThis will put your login info in a netrc file located at:\n\nearthdatalogin:::edl_netrc_path()\n\n[1] \"/home/jovyan/.local/share/R/earthdatalogin/netrc\"\n\n\nYou can open a terminal and run cat /home/jovyan/.local/share/R/earthdatalogin/netrc to see that it has your username and login.\nOnce your netrc file is saved, you can use earthdatalogin::edl_netrc() to authenticate.\n\nearthdatalogin::edl_netrc()\n\nFor the purposes of this workshop, edl_netrc() will work by using a default public account login. Feel free to login with your own [NASA Earthdata account](https://urs.earthdata.nasa.gov/home.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#search-for-data",
    "href": "tutorials/r/1-earthdatalogin.html#search-for-data",
    "title": "Earthdata Search and Discovery",
    "section": "Search for data",
    "text": "Search for data\nThere are multiple keywords we can use to discover data from collections. The table below contains the short_name, concept_id, and doi for some collections we are interested in for the tutorials today. Each of these can be used to search for data or information related to the collection we are interested in.\n\n\n\n\n\n\n\n\nShortname\nCollection Concept ID\nDOI\n\n\n\n\nMUR-JPL-L4-GLOB-v4.1\nC1996881146-POCLOUD\n10.5067/GHGMR-4FJ04\n\n\nAVHRR_OI-NCEI-L4-GLOB-v2.1\nC2036881712-POCLOUD\n10.5067/GHAAO-4BC21\n\n\n\nHow can we find the shortname, concept_id, and doi for collections not in the table above? Let’s take a quick detour:\n\nNavigate to https://search.earthdata.nasa.gov/search 2: Search for “GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis” or click this link (screenshot below).\n\n\nIf we hover over the top box, find and click on the more information button (an i with a circle around it). On this page, you will see the DOI. Now click “View More Info” to get to https://cmr.earthdata.nasa.gov/search/concepts/C1996881146-POCLOUD.html.\nOn that page you will see the “short name” MUR-JPL-L4-GLOB-v4.1. Note the short name was also on the first search page (though it wasn’t labeled as the short name, there).\n\nSearch by short name\n\nshort_name &lt;- 'MUR-JPL-L4-GLOB-v4.1'\n\nLet’s set some time bounds.\n\ntbox &lt;- c(\"2020-01-16\", \"2020-12-16\")\n\nAnd now we search!\n\nresults &lt;- earthdatalogin::edl_search(\n    short_name = short_name,\n    version = \"4.1\",\n    temporal = tbox\n)\nlength(results) # how many links were returned\n\n[1] 336\n\nresults[1:3] # let's see the first 3 of these links\n\n[1] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200116090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n[2] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200117090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n[3] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200118090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n\n\nIn this example we used the short_name parameter to search from our desired data set. However, there are multiple ways to specify the collection(s) we are interested in. Alternative parameters include:\n\ndoi: request collection by digital object identifier (e.g., doi = '10.5067/GHAAO-4BC21')\n\nNOTE: Each Earthdata collect has a unique concept_id and doi. This is not the case with short_name, which can be associated with multiple versions of a collection. If multiple versions of a collection are publicly available, using the short_name parameter with return all versions available. It is advised to use the version parameter in conjunction with the short_name parameter with searching.\nWe can refine our search by passing more parameters that describe the spatiotemporal domain of our use case. Here, we use the temporal parameter to request a date range and the bounding_box parameter to request granules that intersect with a bounding box.\n\nbbox &lt;- c(xmin=-73.5, ymin=33.5, xmax=-43.5, ymax=43.5) \nbbox\n\n xmin  ymin  xmax  ymax \n-73.5  33.5 -43.5  43.5 \n\n\n\nresults &lt;- earthdatalogin::edl_search(\n    short_name = short_name,\n    version = \"4.1\",\n    temporal = tbox,\n    bounding_box = paste(bbox,collapse=\",\")\n)\nlength(results)\n\n[1] 336\n\nresults[1:3]\n\n[1] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200116090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n[2] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200117090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"\n[3] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20200118090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc\"",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#working-with-earthdatalogin-returns",
    "href": "tutorials/r/1-earthdatalogin.html#working-with-earthdatalogin-returns",
    "title": "Earthdata Search and Discovery",
    "section": "Working with earthdatalogin returns",
    "text": "Working with earthdatalogin returns\nFollowing the search for data, you’ll likely take one of two pathways with those results. You may choose to download the assets that have been returned to you or you may choose to continue working with the search results within the R environment.\n\nDownload earthdatalogin results\nIn some cases you may want to download your assets. The earthdatalogin::edl_download() function makes downloading the data from the search results very easy. We won’t download the MUR SST file for this tutorial because it is 673 Gb, but you could with the code below, if inclined.\n\nearthdatalogin::edl_download(\n    results[1],\n    dest = here::here(\"test.nc\")\n)\n\n\n\nWork in the cloud\nWe do not have to download the data to work with it or at least not until we need to compute with it or plot it. Let’s look at a smaller dataset.\n\noi &lt;- earthdatalogin::edl_search(\n    short_name = \"AVHRR_OI-NCEI-L4-GLOB-v2.1\",\n    version = \"2.1\",\n    temporal = c(\"2020-01-16\", \"2020-01-17\")\n)\noi\n\n[1] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20200115120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc\"\n[2] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20200116120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc\"\n[3] \"https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20200117120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc\"\n\n\nLet’s try plotting this. I am going to authenticate again just to make sure my token did not expire. To search, we don’t need to authenticate, but to plot or download, we do.\n\n# Re-authenticate (just in case)\nearthdatalogin::edl_netrc()\n\n\nlibrary(terra)\nras &lt;- terra::rast(x = oi[1], vsi=TRUE)\nplot(ras)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\n\nIf you get the following error:\n\nError: [rast] file does not exist: /vsicurl/https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20200115120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc\n\nIt is likely because you do not have the End User Licence Agreement (EULA)/permissions to use that data set or are not properly logged in using earthdatalogin::edl_netrc(). Another reason may be that your GDAL installation is not properly handling netCDF files.\n\n\nAlso try this example script from the ?earthdatalogin::edl_netrc documentation that uses a .tif file instead of .netCDF.\n\nurl &lt;- earthdatalogin::lpdacc_example_url()\nras &lt;- terra::rast(url, vsi=TRUE)\nplot(ras)\n\n\n\n\n\n\n\n\nHow to you accept EULA’s? Go to https://urs.earthdata.nasa.gov/profile. Look for the EULA tab and accept the one that looks likely. Unfortunately, it is hard to find out which one you need. You can just accept all of them (or all that look possible). Then try your code again.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/1-earthdatalogin.html#conclusion",
    "href": "tutorials/r/1-earthdatalogin.html#conclusion",
    "title": "Earthdata Search and Discovery",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes tutorial 1. You have worked with remote-sensing data in the cloud and plotted it. Way to go!\nNext we will learn to subset the data so we can work with bigger datasets in the cloud without downloading the whole dataset.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Search"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#summary",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#summary",
    "title": "Extract data within a boundary",
    "section": "Summary",
    "text": "Summary\nIn this example, we will utilize the earthdatalogin R package to retrieve sea surface temperature data from NASA Earthdata search. The earthdatalogin R package simplifies the process of discovering and accessing NASA Earth science data.\nThis example is adapted from the NOAA CoastWatch Satellite Data Tutorials. To explore the full range of tutorials on accessing and utilizing oceanographic satellite data, visit the NOAA CoastWatch Tutorial Github repository.\nFor more on earthdatalogin visit the earthdatalogin GitHub page and/or the earthdatalogin documentation site. Be aware that earthdatalogin is under active development and that we are using the development version on GitHub.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#terminology",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#terminology",
    "title": "Extract data within a boundary",
    "section": "Terminology",
    "text": "Terminology\n\nshapefiles: is a simple, nontopological format for storing the geometric location and attribute information of geographic features. Geographic features in a shapefile can be represented by points, lines, or polygons (areas). Learn more here.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#prerequisites",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#prerequisites",
    "title": "Extract data within a boundary",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe tutorials today can be run with the guest Earthdata Login that is in earthdatalogin. However, if you will be using the NASA Earthdata portal more regularly, please register for an Earthdata Login account. Please https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\nImport Required Packages\nNote: See the set-up tab (in left nav bar) for instructions on getting set up on your own computer, but be aware that getting it is common to run into trouble getting GDAL set up properly to handle netCDF files. Using a Docker image (and Python) is often less aggravating.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#datasets-used",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#datasets-used",
    "title": "Extract data within a boundary",
    "section": "Datasets used",
    "text": "Datasets used\nGHRSST Level 4 AVHRR_OI Global Blended Sea Surface Temperature Analysis (GDS2) from NCEI\nThis NOAA blended SST is a moderate resolution satellite-based gap-free sea surface temperature (SST) product. We will use the daily data. https://cmr.earthdata.nasa.gov/search/concepts/C2036881712-POCLOUD.html\nLonghurst Marine Provinces\nThe dataset represents the division of the world oceans into provinces as defined by Longhurst (1995; 1998; 2006). This division has been based on the prevailing role of physical forcing as a regulator of phytoplankton distribution.\nThe Longhurst Marine Provinces dataset is available online (https://www.marineregions.org/downloads.php) and within the shapes folder associated with this repository. For this exercise we will use the Gulf Stream province (ProvCode: GFST)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#load-packages",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#load-packages",
    "title": "Extract data within a boundary",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(terra)\nlibrary(earthdatalogin)\nlibrary(sf)\nlibrary(ggplot2)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#load-boundary-coordinates",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#load-boundary-coordinates",
    "title": "Extract data within a boundary",
    "section": "Load boundary coordinates",
    "text": "Load boundary coordinates\nThe shapefile for the Longhurst marine provinces includes a list of regions.\nFor this exercise, we will only use the boundary of one province, the Gulf Stream region (“GFST”).\n\n# Set directory path for shapefile\ndir_path &lt;- '../resources/longhurst_v4_2010/'\n\n# Import shape files (Longhurst coordinates)\nshapes &lt;- read_sf(dsn = dir_path, layer = \"Longhurst_world_v4_2010\")\n\n# Example List of all the province names\nshapes$ProvCode\n\n [1] \"BPLR\" \"ARCT\" \"SARC\" \"NADR\" \"GFST\" \"NASW\" \"NATR\" \"WTRA\" \"ETRA\" \"SATL\"\n[11] \"NECS\" \"CNRY\" \"GUIN\" \"GUIA\" \"NWCS\" \"MEDI\" \"CARB\" \"NASE\" \"BRAZ\" \"FKLD\"\n[21] \"BENG\" \"MONS\" \"ISSG\" \"EAFR\" \"REDS\" \"ARAB\" \"INDE\" \"INDW\" \"AUSW\" \"BERS\"\n[31] \"PSAE\" \"PSAW\" \"KURO\" \"NPPF\" \"NPSW\" \"TASM\" \"SPSG\" \"NPTG\" \"PNEC\" \"PEQD\"\n[41] \"WARM\" \"ARCH\" \"ALSK\" \"CCAL\" \"CAMR\" \"CHIL\" \"CHIN\" \"SUND\" \"AUSE\" \"NEWZ\"\n[51] \"SSTC\" \"SANT\" \"ANTA\" \"APLR\"\n\n# Get boundary coordinates for Gulf Stream region (GFST)\nGFST &lt;- shapes[shapes$ProvCode == \"GFST\",]\n\nxcoord &lt;- st_coordinates(GFST)[,1]\nycoord &lt;- st_coordinates(GFST)[,2]",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#search-data-from-nasa-earthdata-with-the-dataset-unique-name-and-coordinatesdates",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#search-data-from-nasa-earthdata-with-the-dataset-unique-name-and-coordinatesdates",
    "title": "Extract data within a boundary",
    "section": "Search data from NASA Earthdata with the dataset unique name and coordinates/dates",
    "text": "Search data from NASA Earthdata with the dataset unique name and coordinates/dates\nFirst, connect to NASA Earthdata with no credentials\n\nearthdatalogin::edl_netrc()\n\nThen, define your search and cropping criteria\n\n# Dataset unique name\nshort_name &lt;- 'AVHRR_OI-NCEI-L4-GLOB-v2.1'\n\n# Set boundaries based on the shapefile\nbbox &lt;- c(xmin=min(xcoord), ymin=min(ycoord), xmax=max(xcoord), ymax=max(ycoord)) \n\n# Set time range\ntbox &lt;- c(\"2020-01-01\", \"2020-04-01\")\n\n# Search data that match the boundaries and time range\nresults &lt;- edl_search(\n  short_name = short_name,\n  version = \"2.1\",\n  temporal = tbox,\n  bounding_box = paste(bbox, collapse = \",\")\n)\n\n# Check number of files \nlength(results)\n\n[1] 93\n\n\nThere are 93 files.",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#apply-shapefiles-as-mask-to-satellite-data",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#apply-shapefiles-as-mask-to-satellite-data",
    "title": "Extract data within a boundary",
    "section": "Apply shapefiles as mask to satellite data",
    "text": "Apply shapefiles as mask to satellite data\n\n# Select the first result\nras &lt;- terra::rast(results[1], vsi = TRUE)\n\n# Extract SST from the multi-layer raster data\nras_sst &lt;- ras[[\"analysed_sst\"]]\n\n# Vectorize shapes\nshp &lt;- vect(shapes)\n\n# Get boundaries for GFST\nGFST &lt;- shp[shp$ProvCode == \"GFST\",]\n\nPlot the SST data.\n\nplot(ras_sst)\n\n\n\n\n\n\n\n\nPlot GFST boundaries from shapefile.\n\nplot(GFST,col='red')\n\n\n\n\n\n\n\n\nMask SST with the GFST boundaries and plot.\n\nmasked_rc &lt;- mask(ras_sst, GFST)\n\n# Visualize the SST in GFST Province and crop to the GFST extent\nplot(masked_rc, ext = GFST)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#compute-monthly-average-of-sst",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#compute-monthly-average-of-sst",
    "title": "Extract data within a boundary",
    "section": "Compute monthly average of SST",
    "text": "Compute monthly average of SST\nWe will construct a data cube to compute monthly average for sea surface temperature data within the boundary. To minimize data loading times, the first 10 results, which correspond to approximately two months of data, will be used for this exercise.\nSelect the SST results for end of January and beginning of February.\n\nras_all &lt;- terra::rast(results[c(25:35)], vsi = TRUE)\n\nTrim the SST data to the boundaries of GFST.\n\nrc_all &lt;- terra::mask(ras_all, GFST)\n\nSelect SST data.\n\nrc_sst &lt;- rc_all[\"analysed_sst\", ]\n\nCalculate monthly SST means.\n\n# Function to convert times to year-month format\nyear_month &lt;- function(x) {\n  format(as.Date(time(x), format=\"%Y-%m-%d\"), \"%Y-%m\")\n}\n\n# Convert time to Year-month format for aggregation\nym &lt;- year_month(rc_sst)\n\n# Compute raster mean grouped by Year-month\nmonthly_mean_rast &lt;- terra::tapp(rc_sst, ym, fun = mean)\n\n# Compute mean across raster grouped by Year-month\nmonthly_means &lt;- global(monthly_mean_rast, fun = mean, na.rm=TRUE)",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#convert-raster-into-data-frame",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#convert-raster-into-data-frame",
    "title": "Extract data within a boundary",
    "section": "Convert raster into data frame",
    "text": "Convert raster into data frame\n\n# Convert raster into data.frame\nmonthly_means_df &lt;- as.data.frame(monthly_means)\n\n# Convert year_month to a column\nmonthly_means_df$year_month &lt;- sub(\"X\", \"\", rownames(monthly_means_df))",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#plot-monthly-mean-of-sea-surface-temperature-within-gfst-province",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#plot-monthly-mean-of-sea-surface-temperature-within-gfst-province",
    "title": "Extract data within a boundary",
    "section": "Plot monthly mean of sea surface temperature within GFST province",
    "text": "Plot monthly mean of sea surface temperature within GFST province\n\nggplot(data = monthly_means_df, aes(x = year_month, y = mean, group = 1)) +\n  geom_line() +\n  geom_point() +\n  xlab(\"Year.Month\") + \n  ylab(\"Mean SST (F)\")",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  },
  {
    "objectID": "tutorials/r/3-extract-satellite-data-within-boundary.html#troubleshooting",
    "href": "tutorials/r/3-extract-satellite-data-within-boundary.html#troubleshooting",
    "title": "Extract data within a boundary",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nTroubleshooting\n\n\n\nIf you get the following error:\n\nWarning: Opening a /vsi file with the netCDF driver requires Linux userfaultfd to be available. Or you may set the GDAL_SKIP=netCDF configuration option to force the use of the HDF5 driver. (GDAL error 1)Error: [rast] file does not exist: /vsicurl/https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20191231120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc\n\n[insert solve]",
    "crumbs": [
      "JupyterHub",
      "Tutorials",
      "Tutorials in R",
      "Mask to a shapefile"
    ]
  }
]